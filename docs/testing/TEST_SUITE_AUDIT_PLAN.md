# Test Suite Audit Plan - SpaceX-Level Efficiency

**Mission**: Achieve 100% essential coverage, 0% bloat, NECESSARY-compliant test suite
**Standard**: Every test must earn its place. If it doesn't prevent regressions or validate critical behavior, it's cut.

---

## üéØ NECESSARY Test Criteria

Every test must satisfy **all 7 criteria**:

### **N - Necessary** ‚úÖ
- **Tests critical behavior** that would cause production failures if broken
- **Validates edge cases** that have caused real bugs
- **Prevents regressions** on fixed issues
- ‚ùå **NOT**: Testing framework internals, obvious getters/setters, trivial code

### **E - Explicit** ‚úÖ
- **Clear test name** describes what's being tested (not "test_function_1")
- **Obvious assertions** - anyone can understand what passed/failed
- **Single responsibility** - one concept per test
- ‚ùå **NOT**: Vague names, multiple unrelated assertions, "test_various_things"

### **C - Complete** ‚úÖ
- **Setup + Execute + Assert** (AAA pattern)
- **All edge cases** covered (empty inputs, max values, errors)
- **Teardown** if needed (cleanup resources)
- ‚ùå **NOT**: Partial tests, missing assertions, no cleanup

### **E - Efficient** ‚úÖ
- **Fast execution** (<100ms for unit tests, <5s for integration)
- **Minimal dependencies** (mock external services)
- **Runs in parallel** (no shared state conflicts)
- ‚ùå **NOT**: Slow network calls, file I/O in unit tests, flaky timing dependencies

### **S - Stable** ‚úÖ
- **100% deterministic** (same input ‚Üí same output)
- **No flakiness** (passes every time, not 99% of the time)
- **Environment-independent** (works on CI, Mac, Linux, Windows)
- ‚ùå **NOT**: Timing-dependent, random data without seeds, environment-specific

### **S - Scoped** ‚úÖ
- **Unit tests** test ONE unit (function/class) in isolation
- **Integration tests** test component interactions
- **E2E tests** test full workflows
- ‚ùå **NOT**: Unit tests calling databases, integration tests mocking everything

### **A - Actionable** ‚úÖ
- **Failure = clear bug location** (not "something broke somewhere")
- **Error message** tells you what to fix
- **Fast feedback** (fail immediately when assertion fails)
- ‚ùå **NOT**: Cryptic failures, "AssertionError" with no context

### **R - Relevant** ‚úÖ
- **Production code exists** (no tests for deleted/deprecated code)
- **Behavior matters** (tests user-facing behavior, not implementation details)
- **Current architecture** (updated when refactored)
- ‚ùå **NOT**: Tests for removed features, testing private methods, outdated assertions

### **Y - Yieldful** ‚úÖ
- **Provides value** > cost of maintenance
- **Catches real bugs** (has failed at least once)
- **Documents behavior** (serves as living specification)
- ‚ùå **NOT**: Never-failing tests, duplicate coverage, testing the test framework

---

## üìä Current Test Suite Analysis

### Metrics (Baseline)
```bash
Total Tests: 3,254 (2,922 passed + 9 failed + 323 skipped)
Pass Rate: 99.6% (Article II target: 100%)
Runtime: ~140s (2m20s) on Python 3.13
Coverage: TBD (will measure)
```

### Known Issues
1. **9 Failing Tests** (blocking CI):
   - `tests/trinity_protocol/test_parameter_tuning.py` (3 failures)
   - `tests/trinity_protocol/test_pattern_detector_ambient.py` (2 failures)
   - `tests/unit/shared/test_message_bus.py` (1 failure)
   - `tests/unit/tools/test_tool_cache.py` (2 failures)

2. **323 Skipped Tests**:
   - **Question**: Are these skipped for good reason or abandoned?
   - **Action**: Audit each skip - fix, remove, or document why skipped

3. **Potential Bloat**:
   - 3,254 tests is high - possible duplicate coverage
   - Some tests may be testing implementation details vs behavior

---

## üîç Audit Methodology

### Phase 1: Test Inventory (Day 1)
```bash
# 1. Generate test inventory
pytest --collect-only --quiet > test_inventory.txt

# 2. Categorize by type
grep "test_" test_inventory.txt | \
  awk -F'::' '{print $1}' | \
  sort | uniq -c | sort -rn > test_distribution.txt

# 3. Identify slow tests
pytest --durations=50 > slow_tests.txt

# 4. Find flaky tests (from git history)
git log --all --grep="flaky\|skip\|xfail" --oneline > flaky_history.txt
```

**Output**:
- `test_inventory.txt` - All 3,254 tests listed
- `test_distribution.txt` - Tests per module
- `slow_tests.txt` - Top 50 slowest tests
- `flaky_history.txt` - Tests that have been skipped/marked flaky

### Phase 2: NECESSARY Scoring (Day 2-3)

**Automated Analysis**:
```python
# Script: tools/test_audit/necessary_scorer.py

def score_test(test_path, test_code):
    """Score test 0-7 based on NECESSARY criteria."""
    score = 0

    # N - Necessary: Check if tests critical code path
    if has_critical_assertions(test_code):
        score += 1

    # E - Explicit: Check test name clarity
    if is_explicit_name(test_path):
        score += 1

    # C - Complete: Check AAA pattern
    if has_complete_aaa(test_code):
        score += 1

    # E - Efficient: Check runtime
    if runtime < 100ms:  # unit test threshold
        score += 1

    # S - Stable: Check flaky history
    if not in_flaky_history(test_path):
        score += 1

    # S - Scoped: Check mocking vs real dependencies
    if is_properly_scoped(test_code):
        score += 1

    # A - Actionable: Check assertion messages
    if has_actionable_assertions(test_code):
        score += 1

    # R - Relevant: Check if production code exists
    if has_corresponding_code(test_path):
        score += 1

    # Y - Yieldful: Check git blame for failures
    if has_caught_bugs(test_path):
        score += 1

    return score  # 0-7 (7 = perfect NECESSARY compliance)
```

**Manual Review** (for borderline cases):
- Score 0-3: **DELETE** (bloat, not earning its place)
- Score 4-5: **REVIEW** (needs improvement or deletion)
- Score 6-7: **KEEP** (essential, well-written)

### Phase 3: Test Classification (Day 3-4)

**Category A: Essential (Keep & Fix)**
- Tests critical business logic
- Tests edge cases that caused real bugs
- Tests constitutional compliance
- Tests external API contracts

**Category B: Bloat (Delete)**
- Duplicate coverage (multiple tests for same behavior)
- Testing framework internals
- Testing trivial code (getters/setters without logic)
- Tests for deleted/deprecated code
- Never-failing tests (always pass, provide no value)

**Category C: Broken (Fix or Delete)**
- Currently failing (9 tests)
- Skipped without good reason
- Flaky (timing-dependent, random failures)
- Outdated (testing old implementation)

**Decision Tree**:
```
Is test failing?
‚îú‚îÄ Yes ‚Üí Does it test critical behavior?
‚îÇ         ‚îú‚îÄ Yes ‚Üí FIX (Category A)
‚îÇ         ‚îî‚îÄ No  ‚Üí DELETE (Category C)
‚îî‚îÄ No  ‚Üí Is it NECESSARY (score ‚â•6)?
          ‚îú‚îÄ Yes ‚Üí KEEP (Category A)
          ‚îú‚îÄ Maybe ‚Üí IMPROVE or DELETE (Category B)
          ‚îî‚îÄ No  ‚Üí DELETE (Category B)
```

---

## üõ†Ô∏è Execution Plan

### Week 1: Audit & Categorization

#### Day 1: Inventory
- [x] Run test collection
- [ ] Generate test distribution report
- [ ] Identify slow tests (>1s)
- [ ] Review skip/xfail history

#### Day 2-3: NECESSARY Scoring
- [ ] Run automated NECESSARY scorer
- [ ] Manual review of borderline tests (score 4-5)
- [ ] Create categorized lists:
  - `essential_tests.txt` (Keep)
  - `bloat_tests.txt` (Delete)
  - `broken_tests.txt` (Fix or Delete)

#### Day 4: Coverage Analysis
- [ ] Run coverage report: `pytest --cov --cov-report=html`
- [ ] Identify uncovered critical code
- [ ] Flag redundant coverage (5+ tests for same line)

#### Day 5: Documentation
- [ ] Document findings in `TEST_AUDIT_REPORT.md`
- [ ] Create test removal justification document
- [ ] Get stakeholder approval for deletions

### Week 2: Implementation

#### Day 6-7: Fix Broken Essential Tests
**Priority Order**:
1. Fix 9 currently failing tests (blocking CI)
2. Fix flaky tests (convert to stable or delete)
3. Update outdated tests (new API signatures)

**Fix Workflow**:
```bash
# For each broken test:
1. Understand what it's testing
2. Is the behavior still needed?
   ‚îú‚îÄ Yes ‚Üí Fix test to match current implementation
   ‚îî‚îÄ No  ‚Üí Delete test

3. If fixing:
   - Update assertions
   - Fix mocks/fixtures
   - Ensure NECESSARY compliance
   - Verify passes locally
   - Commit: "fix(test): <description>"

4. If deleting:
   - Verify coverage maintained by other tests
   - Document reason in commit
   - Commit: "chore(test): Remove non-essential test - <reason>"
```

#### Day 8-9: Remove Bloat
```bash
# Delete tests from bloat_tests.txt
for test in $(cat bloat_tests.txt); do
  git rm "$test"
done

# Verify coverage not dropped
pytest --cov --cov-fail-under=95

# Commit batch deletion
git commit -m "chore(test): Remove 500+ bloat tests - NECESSARY audit

Removed tests failing NECESSARY criteria:
- Duplicate coverage: 300 tests
- Trivial code: 150 tests
- Deleted features: 50 tests

Coverage maintained: 95% ‚Üí 95%
Runtime improved: 140s ‚Üí 60s"
```

#### Day 10: Fill Coverage Gaps
```bash
# For each uncovered critical path:
1. Write NECESSARY-compliant test
2. Follow AAA pattern
3. Ensure fast (<100ms)
4. Add to test suite
5. Commit: "test: Add coverage for <critical_behavior>"
```

---

## üìà Success Metrics

### Before Audit
```
Total Tests: 3,254
Pass Rate: 99.6%
Runtime: 140s
Coverage: ~90% (estimated)
Flaky Tests: ~15
Bloat: Unknown
```

### After Audit (Target)
```
Total Tests: ~1,500-2,000 (40-50% reduction)
Pass Rate: 100% ‚úÖ
Runtime: <60s (57% improvement)
Coverage: ‚â•95% (essential code only)
Flaky Tests: 0 ‚úÖ
Bloat: 0 ‚úÖ
NECESSARY Score: 7/7 average
```

### Key Performance Indicators
- ‚úÖ **100% Pass Rate** (Article II compliance)
- ‚úÖ **0 Flaky Tests** (Stable criterion)
- ‚úÖ **<60s Runtime** (Efficient criterion)
- ‚úÖ **‚â•95% Coverage** (Complete criterion)
- ‚úÖ **NECESSARY Score ‚â•6** (All criteria)

---

## üöÄ Implementation Phases

### Phase 1: Emergency Triage (Now)
**Goal**: Unblock CI, get to 100% pass rate

**Actions**:
1. Fix or skip 9 failing tests
2. Merge PR #16 (snapshot system)
3. Restore green main branch

**Timeline**: 2-4 hours
**Owner**: Autonomous Agent

### Phase 2: Full Audit (Week 1)
**Goal**: Complete NECESSARY analysis

**Actions**:
1. Run automated scorer
2. Categorize all 3,254 tests
3. Get approval for deletions
4. Document findings

**Timeline**: 5 days
**Owner**: Chief Architect + Auditor Agent

### Phase 3: Cleanup (Week 2)
**Goal**: Execute audit recommendations

**Actions**:
1. Fix broken essential tests
2. Delete bloat tests
3. Fill coverage gaps
4. Validate metrics

**Timeline**: 5 days
**Owner**: Agency Code Agent + Test Generator

### Phase 4: Continuous Enforcement (Ongoing)
**Goal**: Prevent bloat from returning

**Actions**:
1. Add pre-commit hook: NECESSARY check
2. CI gate: New tests must score ‚â•6
3. Monthly audit: Review new tests
4. Documentation: Test writing guide

**Timeline**: Setup 1 day, ongoing monitoring
**Owner**: Quality Enforcer Agent

---

## üõ°Ô∏è Constitutional Compliance

### Article I: Complete Context
- ‚úÖ Full test inventory before deletion
- ‚úÖ Coverage analysis ensures no gaps
- ‚úÖ All tests categorized (no unknowns)

### Article II: 100% Verification
- ‚úÖ Target: 100% pass rate (from 99.6%)
- ‚úÖ All essential tests must pass
- ‚úÖ No flaky tests permitted

### Article III: Automated Enforcement
- ‚úÖ Pre-commit: NECESSARY score check
- ‚úÖ CI: Block tests with score <6
- ‚úÖ No manual overrides

### Article IV: Continuous Learning
- ‚úÖ Document audit findings
- ‚úÖ Learn from deleted test patterns
- ‚úÖ Share insights across agents

### Article V: Spec-Driven
- ‚úÖ Audit plan spec (this document)
- ‚úÖ Traceability: Every deletion justified
- ‚úÖ Living document: Updated during execution

---

## üìã Next Steps

### Immediate (Now)
1. Approve this audit plan
2. Start Phase 1: Fix 9 failing tests
3. Unblock PR #16

### Short-Term (This Week)
1. Run automated NECESSARY scorer
2. Generate test categorization
3. Review and approve deletions

### Long-Term (Next Week)
1. Execute cleanup (delete bloat)
2. Fill coverage gaps
3. Validate 100% pass rate + <60s runtime

---

**Decision Required**:
- ‚úÖ **Approve Audit Plan** - Proceed with SpaceX-level efficiency mission
- ‚è∏Ô∏è **Modify Plan** - What changes needed?
- ‚ùå **Reject Plan** - Different approach?

**Recommendation**: **Approve and start Phase 1 immediately** to unblock development while planning full audit.

---

**Version**: 1.0
**Created**: 2025-10-03
**Status**: AWAITING APPROVAL
**Estimated Impact**: 40-50% test reduction, 57% runtime improvement, 100% pass rate
