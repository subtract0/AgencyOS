"""
Prompt compression utilities for reducing token usage via Anthropic prompt caching.

Constitutional Compliance:
- Article I: Complete context (cached system prompt + variable task)
- Article II: 100% verification (cache validation tests)
- Article IV: Continuous learning (cache hit rate tracking)

Token Savings:
- First call: ~3,500 tokens (2,000 cached + 1,500 task)
- Cached calls: ~1,500 tokens (60% reduction!)
- Cache TTL: 5 minutes (Anthropic ephemeral cache)

Architecture:
1. System prompts are CACHED (constitution, standards, agent role)
2. Task prompts are VARIABLE (user request, context, files)
3. Anthropic caches system prompts for 5 min with cache_control
4. Subsequent calls within cache window transmit only task prompts

Example Usage:
    from shared.prompt_compression import create_compressed_prompt

    prompts = create_compressed_prompt(
        agent_name="planner",
        task="Create implementation plan",
        context={"files": ["spec.md"]}
    )

    # Use with Anthropic client
    response = client.messages.create(
        model="claude-sonnet-4-5",
        system=[{
            "type": "text",
            "text": prompts["system"],
            "cache_control": {"type": "ephemeral"}  # Cache for 5 min
        }],
        messages=[{"role": "user", "content": prompts["task"]}]
    )
"""

from typing import TypedDict, Optional
from pathlib import Path


class CompressedPrompt(TypedDict):
    """
    Compressed prompt structure for caching.

    Attributes:
        system: Cacheable system prompt (constitution, standards, role)
        task: Variable task prompt (user request, context)
    """
    system: str
    task: str


# System prompt template (cached via Anthropic)
SYSTEM_PROMPT_TEMPLATE = """You are {agent_name}, {agent_role}.

## Constitutional Framework (CACHED)
{constitution}

## Quality Standards (CACHED)
{quality_standards}

## Agent-Specific Instructions
{agent_instructions}
"""

# Task prompt template (variable, not cached)
TASK_PROMPT_TEMPLATE = """## Task
{user_request}

## Context
{context}

## Relevant Files
{files}
"""

# Constitutional core (loaded once, cached)
CONSTITUTION_CORE = """
**Article I**: Complete Context Before Action
**Article II**: 100% Verification and Stability
**Article III**: Automated Merge Enforcement
**Article IV**: Continuous Learning and Improvement
**Article V**: Spec-Driven Development

Full text at `/Users/am/Code/Agency/constitution.md`
"""

# Quality standards (loaded once, cached)
QUALITY_STANDARDS = """
1. TDD is Mandatory (tests first)
2. Strict Typing Always (no Any/Dict[Any, Any])
3. Validate All Inputs (Zod/Pydantic)
4. Repository Pattern (no direct DB access)
5. Result<T,E> Pattern (functional error handling)
6. Functions <50 lines (focused, single purpose)
7. Document Public APIs (JSDoc/docstrings)
8. Lint Before Commit
"""


def load_agent_role(agent_name: str) -> str:
    """
    Load agent role description.

    Returns compressed role definition from agent instruction file.
    Falls back to generic role if file not found.

    Args:
        agent_name: Name of agent (e.g., "planner", "coder")

    Returns:
        Role description string
    """
    try:
        from shared.instruction_loader import load_agent_instruction
        instruction = load_agent_instruction(agent_name)

        # Extract first paragraph (role summary)
        lines = instruction.split('\n')
        for line in lines:
            if line.strip() and not line.startswith('#'):
                return line.strip()

        return f"specialized agent for {agent_name} tasks"
    except Exception:
        return f"specialized agent for {agent_name} tasks"


def load_agent_instructions(agent_name: str) -> str:
    """
    Load agent-specific instructions.

    Loads compressed instructions from .claude/agents/{agent_name}.md
    Uses instruction_loader cache for efficiency.

    Args:
        agent_name: Name of agent

    Returns:
        Agent-specific instructions (compressed)
    """
    try:
        from shared.instruction_loader import load_agent_instruction
        return load_agent_instruction(agent_name)
    except Exception:
        return f"No specific instructions for {agent_name}"


def create_compressed_prompt(
    agent_name: str,
    task: str,
    context: Optional[dict] = None
) -> CompressedPrompt:
    """
    Create compressed prompt with caching support.

    Splits prompt into:
    - System (cacheable): Constitution, standards, agent role/instructions
    - Task (variable): User request, context, files

    Token Estimates:
    - System: ~2,000 tokens (cached for 5 min)
    - Task: ~1,500 tokens (transmitted each call)
    - Total first call: ~3,500 tokens
    - Total cached call: ~1,500 tokens (60% savings!)

    Args:
        agent_name: Name of agent (e.g., "planner", "coder")
        task: User task/request description
        context: Optional context dict with 'files', 'data', etc.

    Returns:
        CompressedPrompt with 'system' and 'task' fields

    Example:
        >>> prompts = create_compressed_prompt(
        ...     agent_name="planner",
        ...     task="Create implementation plan for user auth",
        ...     context={"files": ["spec.md", "plan.md"]}
        ... )
        >>> len(prompts["system"])  # ~8,000 chars = ~2,000 tokens
        >>> len(prompts["task"])    # ~6,000 chars = ~1,500 tokens
    """
    context = context or {}

    # Load agent metadata
    agent_role = load_agent_role(agent_name)
    agent_instructions = load_agent_instructions(agent_name)

    # Build system prompt (cacheable)
    system_prompt = SYSTEM_PROMPT_TEMPLATE.format(
        agent_name=agent_name,
        agent_role=agent_role,
        constitution=CONSTITUTION_CORE,
        quality_standards=QUALITY_STANDARDS,
        agent_instructions=agent_instructions
    )

    # Build task prompt (variable)
    files_content = ""
    if "files" in context and context["files"]:
        files_content = "\n".join(f"- {f}" for f in context["files"])

    context_content = context.get("data", "") or ""

    task_prompt = TASK_PROMPT_TEMPLATE.format(
        user_request=task,
        context=context_content,
        files=files_content or "(none)"
    )

    return CompressedPrompt(
        system=system_prompt.strip(),
        task=task_prompt.strip()
    )


def estimate_tokens(text: str) -> int:
    """
    Estimate token count for text.

    Simple approximation: 1 token â‰ˆ 4 characters
    (Actual tokenization varies by model, this is conservative)

    Args:
        text: Text to estimate

    Returns:
        Estimated token count
    """
    return len(text) // 4


def get_compression_stats(prompts: CompressedPrompt) -> dict:
    """
    Get compression statistics for prompt.

    Returns token estimates and savings calculations.

    Args:
        prompts: Compressed prompt structure

    Returns:
        Dict with:
        - system_tokens: Estimated tokens in system prompt (cached)
        - task_tokens: Estimated tokens in task prompt
        - first_call_tokens: Total tokens on first call
        - cached_call_tokens: Total tokens on cached call
        - savings_percent: Percentage saved on cached calls
    """
    system_tokens = estimate_tokens(prompts["system"])
    task_tokens = estimate_tokens(prompts["task"])
    first_call = system_tokens + task_tokens
    cached_call = task_tokens  # System is cached

    savings_percent = 0
    if first_call > 0:
        savings_percent = round((1 - cached_call / first_call) * 100, 1)

    return {
        "system_tokens": system_tokens,
        "task_tokens": task_tokens,
        "first_call_tokens": first_call,
        "cached_call_tokens": cached_call,
        "savings_percent": savings_percent,
        "cache_ttl_minutes": 5,
    }
