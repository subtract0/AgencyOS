"""
Comprehensive healing tests for AuditorAgent - Meta-testing the quality auditor.
Addresses critical NECESSARY violations identified in the self-audit.

This file implements the "healing the healer" philosophy by:
1. Testing the auditor's ability to audit itself
2. Comprehensive edge case and error condition testing
3. Performance testing for large codebases
4. Security testing for arbitrary code analysis
5. Meta-recursive quality assessment

Generated by TestGeneratorAgent for Q(T) healing: 0.58 â†’ 0.85+
"""

import os
import json
import tempfile
import asyncio
import time
from pathlib import Path
from unittest.mock import patch

import pytest

from auditor_agent.auditor_agent import (
    AnalyzeCodebase
)
from auditor_agent.ast_analyzer import ASTAnalyzer


class TestAuditorAgentMetaTesting:
    """Meta-testing suite - Testing the auditor's ability to audit itself."""

    @pytest.fixture
    def meta_codebase(self):
        """Create a meta-codebase that includes auditor code for self-testing."""
        temp_dir = tempfile.mkdtemp()

        # Create auditor source file replica
        auditor_source = Path(temp_dir) / "auditor_replica.py"
        auditor_source.write_text('''
class MockAuditor:
    """Mock auditor for meta-testing."""

    def __init__(self):
        self.audit_count = 0

    def audit(self, target):
        """Perform audit operation."""
        self.audit_count += 1
        if not target:
            raise ValueError("Cannot audit empty target")
        return {"qt_score": 0.8, "violations": []}

    async def async_audit(self, target):
        """Async audit operation."""
        await asyncio.sleep(0.001)  # Simulate async work
        return self.audit(target)

    def _private_method(self):
        """Private method should not be tested."""
        return "private"
''')

        # Create corresponding test file
        test_file = Path(temp_dir) / "test_auditor_replica.py"
        test_file.write_text('''
import pytest

def test_mock_auditor_basic():
    """Basic auditor test."""
    from auditor_replica import MockAuditor
    auditor = MockAuditor()
    assert auditor.audit_count == 0

def test_mock_auditor_edge_case():
    """Edge case testing."""
    from auditor_replica import MockAuditor
    auditor = MockAuditor()
    with pytest.raises(ValueError):
        auditor.audit("")

@pytest.mark.asyncio
async def test_mock_auditor_async():
    """Async operation test."""
    from auditor_replica import MockAuditor
    auditor = MockAuditor()
    result = await auditor.async_audit("test")
    assert result["qt_score"] == 0.8
''')

        yield temp_dir

        import shutil
        shutil.rmtree(temp_dir, ignore_errors=True)

    def test_auditor_audits_itself_basic(self, meta_codebase):
        """Test that auditor can audit a codebase that contains audit logic."""
        tool = AnalyzeCodebase(target_path=meta_codebase, mode="full")
        result = tool.run()
        result_data = json.loads(result)

        assert "qt_score" in result_data
        assert result_data["qt_score"] >= 0.0

        # Should detect the mock auditor functions
        codebase_analysis = result_data["codebase_analysis"]
        assert codebase_analysis["total_behaviors"] > 0
        assert codebase_analysis["total_test_functions"] > 0

    def test_meta_recursive_analysis(self, meta_codebase):
        """Test recursive analysis - auditor analyzing audit-like behavior."""
        tool = AnalyzeCodebase(target_path=meta_codebase, mode="verification")

        # First audit
        result1 = tool.run()
        result1_data = json.loads(result1)

        # Second audit of same codebase (should be consistent)
        result2 = tool.run()
        result2_data = json.loads(result2)

        # Results should be deterministic
        assert result1_data["qt_score"] == result2_data["qt_score"]
        assert len(result1_data["violations"]) == len(result2_data["violations"])

    def test_self_reference_paradox_detection(self):
        """Test detection of infinite recursion in self-audit scenarios."""
        with tempfile.TemporaryDirectory() as temp_dir:
            # Create a pathological case: auditor that audits itself
            pathological_file = Path(temp_dir) / "self_audit.py"
            pathological_file.write_text('''
class SelfAuditor:
    def audit_self(self):
        """This method tries to audit itself - potential infinite recursion."""
        return self.audit_self()  # Dangerous recursion
''')

            tool = AnalyzeCodebase(target_path=temp_dir)
            result = tool.run()
            result_data = json.loads(result)

            # Should complete without hanging (no actual infinite recursion in analysis)
            assert "qt_score" in result_data
            # Should detect the problematic behavior
            assert result_data["codebase_analysis"]["total_behaviors"] > 0

    def test_auditor_confidence_assessment(self, meta_codebase):
        """Test Y (Yielding Confidence) property specifically for audit operations."""
        tool = AnalyzeCodebase(target_path=meta_codebase)

        # Mock the necessary analysis to focus on confidence
        with patch.object(tool, '_analyze_necessary_compliance') as mock_necessary:
            mock_necessary.return_value = {
                "N": {"score": 0.9, "violations": []},
                "E": {"score": 0.8, "violations": []},
                "C": {"score": 0.85, "violations": []},
                "E2": {"score": 0.75, "violations": []},
                "S": {"score": 0.8, "violations": []},
                "S2": {"score": 0.7, "violations": []},
                "A": {"score": 0.9, "violations": []},
                "R": {"score": 0.85, "violations": []},
                "Y": {"score": 0.6, "violations": ["Meta-testing confidence needs improvement"]}
            }

            result = tool.run()
            result_data = json.loads(result)

            # Should identify confidence issues in meta-testing
            y_violations = [v for v in result_data["violations"] if v["property"] == "Y"]
            assert len(y_violations) > 0


class TestAuditorAgentEdgeCases:
    """Comprehensive edge case testing for auditor - addressing E property violations."""

    def test_empty_codebase_analysis(self):
        """Test analysis of completely empty codebase."""
        with tempfile.TemporaryDirectory() as empty_dir:
            tool = AnalyzeCodebase(target_path=empty_dir, mode="full")
            result = tool.run()
            result_data = json.loads(result)

            assert result_data["qt_score"] == 0.0
            assert result_data["codebase_analysis"]["total_behaviors"] == 0
            assert result_data["codebase_analysis"]["total_test_functions"] == 0

            # Should have meaningful violations for empty codebase
            assert len(result_data["violations"]) > 0

    def test_single_file_codebase(self):
        """Test analysis of single-file codebase."""
        with tempfile.TemporaryDirectory() as temp_dir:
            single_file = Path(temp_dir) / "single.py"
            single_file.write_text('''
def hello():
    """Single function."""
    return "hello"
''')

            tool = AnalyzeCodebase(target_path=temp_dir)
            result = tool.run()
            result_data = json.loads(result)

            assert result_data["codebase_analysis"]["total_behaviors"] == 1
            assert result_data["codebase_analysis"]["total_test_functions"] == 0
            # Should have low Q(T) score due to no tests
            assert result_data["qt_score"] < 0.5

    def test_massive_function_count(self):
        """Test analysis of file with many functions (boundary testing)."""
        with tempfile.TemporaryDirectory() as temp_dir:
            massive_file = Path(temp_dir) / "massive.py"

            # Generate file with 100 functions
            functions = []
            for i in range(100):
                functions.append(f'''
def function_{i}(param_{i}):
    """Function number {i}."""
    return param_{i} * {i}
''')

            massive_file.write_text('\n'.join(functions))

            tool = AnalyzeCodebase(target_path=temp_dir)
            result = tool.run()
            result_data = json.loads(result)

            assert result_data["codebase_analysis"]["total_behaviors"] == 100
            # Analysis should complete in reasonable time
            assert "qt_score" in result_data

    def test_malformed_python_files(self):
        """Test handling of syntactically invalid Python files."""
        with tempfile.TemporaryDirectory() as temp_dir:
            # Create invalid Python file
            invalid_file = Path(temp_dir) / "invalid.py"
            invalid_file.write_text('''
def broken_function(
    # Missing closing parenthesis and other syntax errors
    return "this won't parse"
    def another_broken
        pass
    ][{invalid syntax
''')

            tool = AnalyzeCodebase(target_path=temp_dir)
            result = tool.run()
            result_data = json.loads(result)

            # Should handle gracefully without crashing
            assert "qt_score" in result_data
            # May have zero behaviors due to parse failure
            assert result_data["codebase_analysis"]["total_behaviors"] >= 0

    def test_unicode_and_special_characters(self):
        """Test handling of files with unicode and special characters."""
        with tempfile.TemporaryDirectory() as temp_dir:
            unicode_file = Path(temp_dir) / "unicode_test.py"
            unicode_file.write_text('''
def Ð¿Ñ€Ð¾Ñ†ÐµÐ´ÑƒÑ€Ð°_Ñ_unicode(Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€):
    """Ð¤ÑƒÐ½ÐºÑ†Ð¸Ñ Ñ unicode ÑÐ¸Ð¼Ð²Ð¾Ð»Ð°Ð¼Ð¸."""
    return f"Ð ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚: {Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€} ðŸŽ‰"

def function_with_emoji():
    """Function with emoji in docstring ðŸš€."""
    return "rocket ship"

class ÐšÐ»Ð°ÑÑÐ¡Ð ÑƒÑÑÐºÐ¸Ð¼Ð˜Ð¼ÐµÐ½ÐµÐ¼:
    """ÐšÐ»Ð°ÑÑ Ñ Ñ€ÑƒÑÑÐºÐ¸Ð¼ Ð¸Ð¼ÐµÐ½ÐµÐ¼."""

    def Ð¼ÐµÑ‚Ð¾Ð´(self):
        return "Ð¼ÐµÑ‚Ð¾Ð´ Ñ€Ð°Ð±Ð¾Ñ‚Ð°ÐµÑ‚"
''', encoding='utf-8')

            tool = AnalyzeCodebase(target_path=temp_dir)
            result = tool.run()
            result_data = json.loads(result)

            # Should handle unicode gracefully - at least parse without error
            assert "codebase_analysis" in result_data
            assert "qt_score" in result_data
            # Unicode might not parse correctly but shouldn't crash
            assert result_data["codebase_analysis"]["total_behaviors"] >= 0

    def test_deeply_nested_directories(self):
        """Test analysis of deeply nested directory structures."""
        with tempfile.TemporaryDirectory() as temp_dir:
            # Create nested structure: temp/a/b/c/d/e/f/g/h/deep.py
            deep_path = Path(temp_dir)
            for letter in "abcdefgh":
                deep_path = deep_path / letter
                deep_path.mkdir()

            deep_file = deep_path / "deep.py"
            deep_file.write_text('''
def deep_function():
    """Function in deeply nested directory."""
    return "deep"
''')

            tool = AnalyzeCodebase(target_path=temp_dir)
            result = tool.run()
            result_data = json.loads(result)

            # Should find the deeply nested file
            assert result_data["codebase_analysis"]["total_behaviors"] >= 1

    def test_binary_and_non_python_files(self):
        """Test handling of non-Python files mixed in codebase."""
        with tempfile.TemporaryDirectory() as temp_dir:
            # Create various file types
            python_file = Path(temp_dir) / "code.py"
            python_file.write_text('def test_func(): return True')

            # Binary file
            binary_file = Path(temp_dir) / "binary.pyc"
            binary_file.write_bytes(b'\x00\x01\x02\x03\x04\x05')

            # Text file with .py extension but invalid Python
            fake_py = Path(temp_dir) / "fake.py"
            fake_py.write_text("This is not Python code at all!")

            # JSON file
            json_file = Path(temp_dir) / "config.json"
            json_file.write_text('{"key": "value"}')

            tool = AnalyzeCodebase(target_path=temp_dir)
            result = tool.run()
            result_data = json.loads(result)

            # Should handle gracefully and analyze valid Python files
            assert "qt_score" in result_data
            # Should find at least the valid Python file
            assert result_data["codebase_analysis"]["total_behaviors"] >= 1


class TestAuditorAgentErrorConditions:
    """Comprehensive error condition testing - addressing E2 property violations."""

    def test_nonexistent_path_handling(self):
        """Test handling of nonexistent analysis targets."""
        tool = AnalyzeCodebase(target_path="/completely/nonexistent/path/12345")
        result = tool.run()
        result_data = json.loads(result)

        assert "error" in result_data
        assert "does not exist" in result_data["error"]

    def test_permission_denied_scenarios(self):
        """Test handling of permission denied errors."""
        if os.name == 'nt':  # Windows
            restricted_path = "C:\\System Volume Information"
        else:  # Unix-like
            restricted_path = "/root/.private"

        tool = AnalyzeCodebase(target_path=restricted_path)
        result = tool.run()
        result_data = json.loads(result)

        # Should handle gracefully (may succeed if permissions allow)
        assert isinstance(result_data, dict)

    def test_circular_symlink_handling(self):
        """Test handling of circular symbolic links."""
        if os.name == 'posix':  # Unix-like systems
            with tempfile.TemporaryDirectory() as temp_dir:
                link_a = Path(temp_dir) / "link_a"
                link_b = Path(temp_dir) / "link_b"

                try:
                    link_a.symlink_to(link_b)
                    link_b.symlink_to(link_a)

                    tool = AnalyzeCodebase(target_path=temp_dir)
                    result = tool.run()
                    result_data = json.loads(result)

                    # Should handle without infinite recursion
                    assert "qt_score" in result_data
                except OSError:
                    # Skip if symlinks not supported
                    pytest.skip("Symlinks not supported")

    def test_out_of_memory_simulation(self):
        """Test behavior under memory pressure (simulated)."""
        with tempfile.TemporaryDirectory() as temp_dir:
            # Create a file that would consume significant memory to parse
            large_file = Path(temp_dir) / "large.py"

            # Generate large but valid Python file
            large_content = []
            for i in range(1000):  # Create many functions
                large_content.append(f'''
def large_function_{i}():
    """Large function {i}."""
    # Simulate complex function with many variables
    {chr(97 + (i % 26))} = {i}
    {chr(97 + ((i + 1) % 26))} = {i + 1}
    {chr(97 + ((i + 2) % 26))} = {i + 2}
    return {chr(97 + (i % 26))} + {chr(97 + ((i + 1) % 26))} + {chr(97 + ((i + 2) % 26))}
''')

            large_file.write_text('\n'.join(large_content))

            tool = AnalyzeCodebase(target_path=temp_dir)

            # Should complete within reasonable time and memory
            start_time = time.time()
            result = tool.run()
            end_time = time.time()

            result_data = json.loads(result)

            # Should complete in reasonable time (less than 30 seconds)
            assert (end_time - start_time) < 30
            assert "qt_score" in result_data

    def test_concurrent_access_scenarios(self):
        """Test behavior when files are modified during analysis."""
        with tempfile.TemporaryDirectory() as temp_dir:
            test_file = Path(temp_dir) / "concurrent.py"
            test_file.write_text('def initial(): return "initial"')

            tool = AnalyzeCodebase(target_path=temp_dir)

            # Simulate file modification during analysis
            with patch('auditor_agent.ast_analyzer.ASTAnalyzer.analyze_file') as mock_analyze:
                def side_effect(file_path):
                    # Modify file during analysis
                    if file_path.endswith('concurrent.py'):
                        Path(file_path).write_text('def modified(): return "modified"')
                    # Return a complete minimal analysis result
                    return {
                        "functions": [],
                        "classes": [],
                        "is_test_file": False,
                        "test_functions": [],
                        "imports": [],
                        "behaviors": 0,
                        "complexity": 1
                    }

                mock_analyze.side_effect = side_effect

                result = tool.run()
                result_data = json.loads(result)

                # Should handle gracefully
                assert "qt_score" in result_data

    def test_ast_parsing_edge_cases(self):
        """Test AST parsing with various edge cases."""
        test_cases = [
            # Empty file
            "",
            # Only comments
            "# This is only a comment\n# Another comment",
            # Only docstring
            '"""Module docstring only."""',
            # Complex expressions
            '''
def complex_function():
    """Complex function with various constructs."""
    x = [i for i in range(10) if i % 2 == 0]
    y = {k: v for k, v in enumerate(x) if k < 5}
    z = lambda a: a ** 2
    return sum(z(i) for i in x)
''',
            # Deeply nested structures
            '''
def deeply_nested():
    """Deeply nested function."""
    if True:
        if True:
            if True:
                for i in range(10):
                    for j in range(10):
                        if i * j > 50:
                            try:
                                result = i / j
                                return result
                            except ZeroDivisionError:
                                continue
    return None
'''
        ]

        for i, code in enumerate(test_cases):
            with tempfile.TemporaryDirectory() as temp_dir:
                test_file = Path(temp_dir) / f"edge_case_{i}.py"
                test_file.write_text(code)

                tool = AnalyzeCodebase(target_path=temp_dir)
                result = tool.run()
                result_data = json.loads(result)

                # Should handle all cases gracefully
                assert "qt_score" in result_data


class TestAuditorAgentAsyncOperations:
    """Async operation testing - addressing A property violations."""

    @pytest.mark.asyncio
    async def test_async_audit_workflow(self):
        """Test auditor in async context (simulated)."""
        with tempfile.TemporaryDirectory() as temp_dir:
            async_file = Path(temp_dir) / "async_code.py"
            async_file.write_text('''
import asyncio

async def async_function():
    """Async function example."""
    await asyncio.sleep(0.1)
    return "async result"

class AsyncClass:
    """Class with async methods."""

    async def async_method(self):
        """Async method."""
        await asyncio.sleep(0.1)
        return "async method result"
''')

            # Simulate async analysis workflow
            tool = AnalyzeCodebase(target_path=temp_dir)

            # Run analysis in async context
            async def async_analysis():
                await asyncio.sleep(0.001)  # Simulate async setup
                result = tool.run()
                await asyncio.sleep(0.001)  # Simulate async cleanup
                return result

            result = await async_analysis()
            result_data = json.loads(result)

            # Should detect async functions properly
            assert result_data["codebase_analysis"]["total_behaviors"] >= 2

            # Check async detection in compliance analysis
            necessary_compliance = result_data["necessary_compliance"]
            assert "A" in necessary_compliance

    def test_async_function_detection_accuracy(self):
        """Test accurate detection of async vs sync functions."""
        with tempfile.TemporaryDirectory() as temp_dir:
            mixed_file = Path(temp_dir) / "mixed_async.py"
            mixed_file.write_text('''
async def async_func1():
    """First async function."""
    return "async1"

def sync_func1():
    """First sync function."""
    return "sync1"

async def async_func2():
    """Second async function."""
    return "async2"

def sync_func2():
    """Second sync function."""
    return "sync2"

class MixedClass:
    """Class with mixed async/sync methods."""

    async def async_method(self):
        """Async method."""
        return "async method"

    def sync_method(self):
        """Sync method."""
        return "sync method"
''')

            tool = AnalyzeCodebase(target_path=temp_dir)
            result = tool.run()
            result_data = json.loads(result)

            # Should properly calculate async coverage
            async_score = result_data["necessary_compliance"]["A"]["score"]
            assert isinstance(async_score, (int, float))
            assert 0.0 <= async_score <= 1.0

    def test_concurrent_analysis_safety(self):
        """Test thread safety during concurrent analysis."""
        import threading
        import queue

        with tempfile.TemporaryDirectory() as temp_dir:
            # Create multiple files for concurrent analysis
            for i in range(5):
                test_file = Path(temp_dir) / f"concurrent_{i}.py"
                test_file.write_text(f'''
def function_{i}():
    """Function {i}."""
    return {i}
''')

            results_queue = queue.Queue()

            def analyze_worker():
                """Worker function for concurrent analysis."""
                tool = AnalyzeCodebase(target_path=temp_dir)
                result = tool.run()
                results_queue.put(result)

            # Start multiple analysis threads
            threads = []
            for _ in range(3):
                thread = threading.Thread(target=analyze_worker)
                threads.append(thread)
                thread.start()

            # Wait for all threads to complete
            for thread in threads:
                thread.join(timeout=30)  # 30 second timeout
                assert not thread.is_alive(), "Analysis thread timed out"

            # Verify all analyses completed successfully
            results = []
            while not results_queue.empty():
                results.append(results_queue.get())

            assert len(results) == 3
            for result in results:
                result_data = json.loads(result)
                assert "qt_score" in result_data


class TestAuditorAgentSideEffects:
    """Side effect testing - addressing S and S2 property violations."""

    def test_file_system_state_preservation(self):
        """Test that analysis doesn't modify file system state."""
        with tempfile.TemporaryDirectory() as temp_dir:
            original_file = Path(temp_dir) / "original.py"
            original_content = '''
def original_function():
    """Original function."""
    return "original"
'''
            original_file.write_text(original_content)

            # Record original state
            original_mtime = original_file.stat().st_mtime
            original_size = original_file.stat().st_size

            # Perform analysis
            tool = AnalyzeCodebase(target_path=temp_dir)
            result = tool.run()
            result_data = json.loads(result)

            # Verify file state unchanged
            assert original_file.exists()
            assert original_file.read_text() == original_content
            assert original_file.stat().st_mtime == original_mtime
            assert original_file.stat().st_size == original_size

            # Analysis should have succeeded
            assert "qt_score" in result_data

    def test_memory_leak_prevention(self):
        """Test that analysis doesn't cause memory leaks."""
        import gc

        with tempfile.TemporaryDirectory() as temp_dir:
            test_file = Path(temp_dir) / "memory_test.py"
            test_file.write_text('''
def memory_function():
    """Function for memory testing."""
    return "memory"
''')

            # Force garbage collection and measure initial memory
            gc.collect()
            initial_objects = len(gc.get_objects())

            # Perform multiple analyses
            for _ in range(10):
                tool = AnalyzeCodebase(target_path=temp_dir)
                result = tool.run()
                result_data = json.loads(result)
                assert "qt_score" in result_data

                # Clear tool reference
                del tool

            # Force garbage collection and measure final memory
            gc.collect()
            final_objects = len(gc.get_objects())

            # Allow some growth but not excessive
            object_growth = final_objects - initial_objects
            assert object_growth < 1000, f"Potential memory leak: {object_growth} objects"

    def test_global_state_isolation(self):
        """Test that analysis doesn't affect global state."""
        import sys

        # Record initial global state
        _ = set(sys.modules.keys())
        initial_path = sys.path.copy()

        with tempfile.TemporaryDirectory() as temp_dir:
            test_file = Path(temp_dir) / "global_test.py"
            test_file.write_text('''
import os
import sys

GLOBAL_VAR = "test_value"

def global_function():
    """Function that might affect global state."""
    global GLOBAL_VAR
    GLOBAL_VAR = "modified"
    return GLOBAL_VAR
''')

            # Perform analysis
            tool = AnalyzeCodebase(target_path=temp_dir)
            result = tool.run()
            result_data = json.loads(result)

            # Verify global state unchanged
            _ = set(sys.modules.keys())
            _ = sys.path.copy()

            # Some new modules may be imported during analysis, but core should be stable
            assert sys.path == initial_path
            assert "qt_score" in result_data

    def test_exception_cleanup(self):
        """Test proper cleanup after exceptions during analysis."""
        with tempfile.TemporaryDirectory() as temp_dir:
            # Create a file that will cause analysis issues
            problematic_file = Path(temp_dir) / "problematic.py"
            problematic_file.write_text('''
def problematic_function():
    """This function has problematic characteristics."""
    # Simulate deeply nested complexity that might cause issues
    return str(eval("1" * 1000))  # Dangerous but for testing
''')

            tool = AnalyzeCodebase(target_path=temp_dir)

            # Mock to simulate exception during analysis
            with patch.object(tool, '_analyze_necessary_compliance') as mock_necessary:
                mock_necessary.side_effect = Exception("Simulated analysis error")

                # Analysis should handle exception gracefully
                try:
                    result = tool.run()
                    # If it returns a result, it should be an error
                    if result:
                        result_data = json.loads(result)
                        # Should contain error information
                        assert isinstance(result_data, dict)
                except Exception as e:
                    # Or it might propagate the exception, which is also acceptable
                    assert "Simulated analysis error" in str(e)


class TestAuditorAgentRegressionPrevention:
    """Regression prevention testing - addressing R property violations."""

    def test_version_compatibility_analysis(self):
        """Test analysis works across different code patterns and Python versions."""
        version_specific_patterns = [
            # Python 3.6+ f-strings
            '''
def f_string_function(name):
    """Function using f-strings."""
    return f"Hello, {name}!"
''',
            # Python 3.8+ walrus operator
            '''
def walrus_function(items):
    """Function using walrus operator."""
    return [item for item in items if (n := len(item)) > 5]
''',
            # Type hints
            '''
from typing import List, Dict, Optional

def typed_function(items: List[str]) -> Optional[Dict[str, int]]:
    """Function with type hints."""
    return {item: len(item) for item in items} if items else None
''',
            # Async context managers
            '''
import asyncio

class AsyncContextManager:
    """Async context manager."""

    async def __aenter__(self):
        await asyncio.sleep(0.001)
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        await asyncio.sleep(0.001)
''',
            # Dataclasses
            '''
from dataclasses import dataclass

@dataclass
class DataPoint:
    """Dataclass example."""
    x: float
    y: float

    def distance(self) -> float:
        """Calculate distance from origin."""
        return (self.x ** 2 + self.y ** 2) ** 0.5
'''
        ]

        for i, pattern in enumerate(version_specific_patterns):
            with tempfile.TemporaryDirectory() as temp_dir:
                test_file = Path(temp_dir) / f"version_pattern_{i}.py"
                test_file.write_text(pattern)

                tool = AnalyzeCodebase(target_path=temp_dir)
                result = tool.run()
                result_data = json.loads(result)

                # Should handle all modern Python patterns
                assert "qt_score" in result_data
                assert result_data["codebase_analysis"]["total_behaviors"] >= 1

    def test_known_issue_patterns(self):
        """Test handling of previously problematic code patterns."""
        problematic_patterns = [
            # Complex list comprehensions
            '''
def complex_comprehension():
    """Complex nested comprehensions."""
    return [
        [y for y in range(x) if y % 2 == 0]
        for x in range(10)
        if x > 5
    ]
''',
            # Multiple inheritance
            '''
class A:
    def method_a(self): pass

class B:
    def method_b(self): pass

class C(A, B):
    """Multiple inheritance."""
    def method_c(self): pass
''',
            # Generator functions
            '''
def generator_function():
    """Generator function."""
    for i in range(100):
        yield i ** 2

def generator_expression_function():
    """Function with generator expression."""
    return (x for x in range(100) if x % 2 == 0)
''',
            # Context managers
            '''
class CustomContextManager:
    """Custom context manager."""

    def __enter__(self):
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        return False
''',
            # Property decorators
            '''
class PropertyClass:
    """Class with properties."""

    def __init__(self):
        self._value = 0

    @property
    def value(self):
        """Value property."""
        return self._value

    @value.setter
    def value(self, val):
        """Value setter."""
        self._value = val
'''
        ]

        for i, pattern in enumerate(problematic_patterns):
            with tempfile.TemporaryDirectory() as temp_dir:
                test_file = Path(temp_dir) / f"problematic_{i}.py"
                test_file.write_text(pattern)

                tool = AnalyzeCodebase(target_path=temp_dir)
                result = tool.run()
                result_data = json.loads(result)

                # Should handle all patterns without regression
                assert "qt_score" in result_data
                # Should detect appropriate number of behaviors
                assert result_data["codebase_analysis"]["total_behaviors"] >= 1

    def test_consistent_scoring_across_runs(self):
        """Test that Q(T) scoring is consistent across multiple runs."""
        with tempfile.TemporaryDirectory() as temp_dir:
            consistent_file = Path(temp_dir) / "consistent.py"
            consistent_file.write_text('''
def consistent_function():
    """Function for consistency testing."""
    return "consistent"

class ConsistentClass:
    """Class for consistency testing."""

    def method(self):
        """Consistent method."""
        return "method"
''')

            test_file = Path(temp_dir) / "test_consistent.py"
            test_file.write_text('''
def test_consistent_function():
    """Test consistent function."""
    from consistent import consistent_function
    assert consistent_function() == "consistent"

def test_consistent_class():
    """Test consistent class."""
    from consistent import ConsistentClass
    obj = ConsistentClass()
    assert obj.method() == "method"
''')

            # Run analysis multiple times
            scores = []
            for _ in range(5):
                tool = AnalyzeCodebase(target_path=temp_dir)
                result = tool.run()
                result_data = json.loads(result)
                scores.append(result_data["qt_score"])

            # All scores should be identical
            assert len(set(scores)) == 1, f"Inconsistent scores: {scores}"

    def test_backward_compatibility_maintenance(self):
        """Test that analysis maintains backward compatibility."""
        # Test legacy code patterns that should still work
        legacy_patterns = [
            # Old-style string formatting
            '''
def old_string_format(name):
    """Old-style string formatting."""
    return "Hello, %s!" % name
''',
            # Classic classes (Python 2 style, but valid in Python 3)
            '''
class LegacyClass:
    """Legacy class definition."""
    pass
''',
            # Old exception handling
            '''
def old_exception_handling():
    """Old-style exception handling."""
    try:
        result = 1 / 0
    except ZeroDivisionError, e:  # This syntax is actually invalid in Python 3
        return "error"
    return result
''',
            # Actually valid legacy pattern
            '''
def valid_legacy():
    """Valid legacy pattern."""
    try:
        result = 1 / 0
    except ZeroDivisionError as e:
        return "error"
    return result
'''
        ]

        for i, pattern in enumerate(legacy_patterns):
            with tempfile.TemporaryDirectory() as temp_dir:
                test_file = Path(temp_dir) / f"legacy_{i}.py"
                test_file.write_text(pattern)

                tool = AnalyzeCodebase(target_path=temp_dir)
                result = tool.run()
                result_data = json.loads(result)

                # Should handle gracefully (may fail parsing for invalid syntax)
                assert "qt_score" in result_data or "error" in result_data


class TestAuditorAgentSecurityConsiderations:
    """Security testing for auditor analyzing arbitrary code."""

    def test_code_injection_prevention(self):
        """Test that auditor doesn't execute analyzed code."""
        with tempfile.TemporaryDirectory() as temp_dir:
            malicious_file = Path(temp_dir) / "malicious.py"
            malicious_file.write_text('''
import os
import sys

# This code should NOT be executed during analysis
os.system("touch /tmp/analysis_vulnerability_test")
sys.exit(1)

def innocent_function():
    """This looks innocent."""
    return "innocent"
''')

            tool = AnalyzeCodebase(target_path=temp_dir)
            result = tool.run()
            result_data = json.loads(result)

            # Analysis should complete without executing the malicious code
            assert "qt_score" in result_data

            # Verify the malicious code wasn't executed
            assert not Path("/tmp/analysis_vulnerability_test").exists()

    def test_path_traversal_prevention(self):
        """Test prevention of path traversal attacks."""
        # Test with path traversal attempts
        dangerous_paths = [
            "../../../etc/passwd",
            "../../../../windows/system32",
            "/etc/shadow",
            "C:\\Windows\\System32\\config\\SAM"
        ]

        for dangerous_path in dangerous_paths:
            tool = AnalyzeCodebase(target_path=dangerous_path)
            result = tool.run()
            result_data = json.loads(result)

            # Should handle safely (likely with error for nonexistent path)
            assert isinstance(result_data, dict)
            # Should not crash or expose sensitive information

    def test_resource_exhaustion_prevention(self):
        """Test prevention of resource exhaustion attacks."""
        with tempfile.TemporaryDirectory() as temp_dir:
            # Create a file designed to consume excessive resources during parsing
            resource_bomb = Path(temp_dir) / "resource_bomb.py"

            # Create deeply nested structure that could cause stack overflow
            nested_content = "def outer():\n"
            for i in range(100):  # Deep nesting
                nested_content += "    " * (i + 1) + f"def inner_{i}():\n"
            nested_content += "    " * 101 + "return 'deep'"

            resource_bomb.write_text(nested_content)

            tool = AnalyzeCodebase(target_path=temp_dir)

            # Should complete within reasonable time and memory limits
            start_time = time.time()
            result = tool.run()
            end_time = time.time()

            # Should not take too long (protection against DoS)
            assert (end_time - start_time) < 60  # 1 minute max

            result_data = json.loads(result)
            assert "qt_score" in result_data

    def test_information_disclosure_prevention(self):
        """Test that analysis doesn't leak sensitive information."""
        with tempfile.TemporaryDirectory() as temp_dir:
            sensitive_file = Path(temp_dir) / "sensitive.py"
            sensitive_file.write_text('''
# Simulate file with sensitive information in comments
# API_KEY = "secret_key_12345"
# PASSWORD = "super_secret_password"
# DATABASE_URL = "postgres://user:pass@host/db"

def process_data():
    """Process sensitive data."""
    secret = "embedded_secret"
    return "processed"
''')

            tool = AnalyzeCodebase(target_path=temp_dir)
            result = tool.run()
            result_data = json.loads(result)

            # Result should not contain sensitive information from comments/strings
            result_str = json.dumps(result_data)
            assert "secret_key_12345" not in result_str
            assert "super_secret_password" not in result_str
            assert "embedded_secret" not in result_str

            # But should still provide useful analysis
            assert "qt_score" in result_data