"""
Comprehensive healing tests for TestGeneratorAgent - Meta-testing the test generator.
Addresses critical NECESSARY violations identified in the self-audit.

This file implements the "test generator testing itself" philosophy by:
1. Testing the generator's ability to generate tests for test-generation logic
2. Meta-validation of generated test quality
3. Self-referential test generation scenarios
4. Comprehensive edge case coverage for test generation
5. Quality assessment of test assessment logic

Generated by TestGeneratorAgent for Q(T) healing: 0.61 â†’ 0.85+
"""

import os
import json
import ast
import tempfile
import asyncio
import time
from pathlib import Path
from unittest.mock import Mock, patch, MagicMock, mock_open
from typing import Dict, List, Any

import pytest

from test_generator_agent.test_generator_agent import (
    create_test_generator_agent,
    GenerateTests
)
from shared.agent_context import create_agent_context


class TestTestGeneratorAgentMetaTesting:
    """Meta-testing suite - Testing the test generator's ability to test itself."""

    @pytest.fixture
    def test_generator_replica_codebase(self):
        """Create a replica of test generator code for meta-testing."""
        temp_dir = tempfile.mkdtemp()

        # Create a mock test generator source file
        generator_source = Path(temp_dir) / "test_generator_replica.py"
        generator_source.write_text('''
import ast
import json
from typing import Dict, List, Any

class MockTestGenerator:
    """Mock test generator for meta-testing."""

    def __init__(self):
        self.generated_tests = []

    def analyze_source(self, source_code: str) -> Dict[str, Any]:
        """Analyze source code for test generation."""
        if not source_code:
            raise ValueError("Cannot analyze empty source")

        try:
            tree = ast.parse(source_code)
            functions = []
            classes = []

            for node in ast.walk(tree):
                if isinstance(node, ast.FunctionDef):
                    functions.append({
                        "name": node.name,
                        "args": [arg.arg for arg in node.args.args]
                    })
                elif isinstance(node, ast.ClassDef):
                    classes.append({"name": node.name})

            return {
                "functions": functions,
                "classes": classes,
                "module_name": "test_module"
            }
        except SyntaxError as e:
            raise ValueError(f"Syntax error in source: {e}")

    def generate_test_for_function(self, func_info: Dict) -> str:
        """Generate test code for a function."""
        func_name = func_info["name"]
        args = func_info.get("args", [])

        test_code = f'''
def test_{func_name}_basic():
    """Test basic functionality of {func_name}."""
    # Arrange
    {self._generate_mock_args(args)}

    # Act
    result = {func_name}({', '.join(f'mock_{arg}' for arg in args if arg != 'self')})

    # Assert
    assert result is not None
'''
        self.generated_tests.append(test_code.strip())
        return test_code.strip()

    def generate_edge_case_test(self, func_info: Dict) -> str:
        """Generate edge case test for a function."""
        func_name = func_info["name"]

        test_code = f'''
def test_{func_name}_edge_cases():
    """Test edge cases for {func_name}."""
    # Test with None
    # Test with empty values
    # Test with boundary conditions
    pass
'''
        self.generated_tests.append(test_code.strip())
        return test_code.strip()

    def generate_error_test(self, func_info: Dict) -> str:
        """Generate error condition test for a function."""
        func_name = func_info["name"]

        test_code = f'''
def test_{func_name}_error_conditions():
    """Test error conditions for {func_name}."""
    import pytest

    with pytest.raises(Exception):
        {func_name}(invalid_input)
'''
        self.generated_tests.append(test_code.strip())
        return test_code.strip()

    def _generate_mock_args(self, args: List[str]) -> str:
        """Generate mock arguments for test."""
        if not args or args == ['self']:
            return "# No arguments needed"

        mock_assignments = []
        for arg in args:
            if arg == 'self':
                continue
            mock_assignments.append(f'mock_{arg} = "test_value"')

        return '\\n    '.join(mock_assignments)

    async def async_generate_tests(self, source_code: str) -> List[str]:
        """Async test generation."""
        await asyncio.sleep(0.001)  # Simulate async work
        analysis = self.analyze_source(source_code)
        tests = []

        for func in analysis["functions"]:
            tests.append(self.generate_test_for_function(func))

        return tests

    def validate_generated_test_quality(self, test_code: str) -> Dict[str, Any]:
        """Validate the quality of generated test code."""
        quality_metrics = {
            "has_docstring": '"""' in test_code,
            "has_assertions": "assert" in test_code,
            "follows_aaa_pattern": all(pattern in test_code for pattern in ["# Arrange", "# Act", "# Assert"]),
            "has_imports": "import" in test_code,
            "valid_syntax": True
        }

        try:
            ast.parse(test_code)
        except SyntaxError:
            quality_metrics["valid_syntax"] = False

        quality_metrics["overall_score"] = sum(quality_metrics.values()) / len(quality_metrics)
        return quality_metrics
''')

        # Create a basic test file for the replica
        test_file = Path(temp_dir) / "test_test_generator_replica.py"
        test_file.write_text('''
import pytest

def test_mock_test_generator_basic():
    """Basic test for mock test generator."""
    from test_generator_replica import MockTestGenerator
    generator = MockTestGenerator()
    assert len(generator.generated_tests) == 0

def test_source_analysis():
    """Test source code analysis."""
    from test_generator_replica import MockTestGenerator
    generator = MockTestGenerator()

    source = "def hello(): return 'hello'"
    analysis = generator.analyze_source(source)

    assert "functions" in analysis
    assert len(analysis["functions"]) == 1
    assert analysis["functions"][0]["name"] == "hello"
''')

        yield temp_dir

        import shutil
        shutil.rmtree(temp_dir, ignore_errors=True)

    def test_generator_generates_tests_for_itself(self, test_generator_replica_codebase):
        """Test that test generator can generate tests for test generation logic."""
        # Create audit report indicating test generation needs testing
        audit_report = {
            "mode": "full",
            "target": test_generator_replica_codebase,
            "qt_score": 0.4,
            "violations": [
                {
                    "property": "N",
                    "severity": "critical",
                    "description": "Test generator lacks comprehensive tests",
                    "recommendation": "Generate tests for test generation logic"
                },
                {
                    "property": "E",
                    "severity": "high",
                    "description": "Edge cases in test generation not covered",
                    "recommendation": "Add edge case tests for test generation"
                }
            ]
        }

        source_file = Path(test_generator_replica_codebase) / "test_generator_replica.py"

        tool = GenerateTests(
            audit_report=json.dumps(audit_report),
            target_file=str(source_file)
        )

        result = tool.run()
        result_data = json.loads(result)

        assert result_data["status"] == "success"
        assert result_data["tests_generated"] > 0
        assert result_data["violations_addressed"] >= 2

        # Verify test file was created
        test_file_path = result_data["test_file"]
        assert os.path.exists(test_file_path)

        # Verify generated tests include meta-testing patterns
        with open(test_file_path, 'r') as f:
            test_content = f.read()

        # Should generate tests for test generation methods
        assert "test_analyze_source" in test_content or "test_generate_test_for_function" in test_content

    def test_meta_quality_assessment(self, test_generator_replica_codebase):
        """Test assessment of test quality by test generation logic."""
        source_file = Path(test_generator_replica_codebase) / "test_generator_replica.py"

        # Mock the source analysis to focus on quality validation method
        tool = GenerateTests(
            audit_report=json.dumps({"violations": [{"property": "N", "severity": "critical"}]}),
            target_file=str(source_file)
        )

        analysis = tool._analyze_source_file(str(source_file))

        # Should detect the validate_generated_test_quality method
        quality_methods = [
            func for func in analysis["functions"]
            if "quality" in func["name"].lower() or "validate" in func["name"].lower()
        ]

        assert len(quality_methods) > 0, "Should detect quality validation methods"

    def test_self_referential_test_generation(self):
        """Test the paradox of testing test generation with test generation."""
        with tempfile.TemporaryDirectory() as temp_dir:
            # Create a minimal test generator that can generate tests for itself
            self_generating_file = Path(temp_dir) / "self_generator.py"
            self_generating_file.write_text('''
def generate_test(function_name):
    """Generate a test for a given function."""
    return f"def test_{function_name}(): assert True"

def test_test_generator():
    """Test the test generator itself."""
    test_code = generate_test("example_function")
    assert "def test_example_function" in test_code
    assert "assert True" in test_code
''')

            audit_report = {
                "violations": [
                    {"property": "Y", "severity": "high", "description": "Self-reference testing needed"}
                ]
            }

            tool = GenerateTests(
                audit_report=json.dumps(audit_report),
                target_file=str(self_generating_file)
            )

            result = tool.run()
            result_data = json.loads(result)

            # Should handle self-reference without infinite recursion
            assert result_data["status"] == "success"

            # Check that tests were generated for the test generator
            test_file_path = result_data["test_file"]
            with open(test_file_path, 'r') as f:
                test_content = f.read()

            assert "test_generate_test" in test_content

    def test_infinite_recursion_prevention(self):
        """Test prevention of infinite recursion in self-testing scenarios."""
        with tempfile.TemporaryDirectory() as temp_dir:
            recursive_file = Path(temp_dir) / "recursive_generator.py"
            recursive_file.write_text('''
def generate_test_for_generator():
    """Generate test for test generator - potential infinite recursion."""
    return generate_test_for_generator()  # Dangerous recursion

def safe_generate_test():
    """Safe test generation method."""
    return "def test_example(): pass"
''')

            audit_report = {
                "violations": [{"property": "R", "severity": "critical", "description": "Recursion safety"}]
            }

            tool = GenerateTests(
                audit_report=json.dumps(audit_report),
                target_file=str(recursive_file)
            )

            # Should complete without hanging
            start_time = time.time()
            result = tool.run()
            end_time = time.time()

            # Should complete quickly (no infinite recursion)
            assert (end_time - start_time) < 10

            result_data = json.loads(result)
            assert result_data["status"] == "success"


class TestTestGeneratorEdgeCases:
    """Comprehensive edge case testing for test generation."""

    def test_empty_source_file_handling(self):
        """Test handling of empty source files."""
        with tempfile.TemporaryDirectory() as temp_dir:
            empty_file = Path(temp_dir) / "empty.py"
            empty_file.write_text("")

            audit_report = {
                "violations": [{"property": "N", "severity": "critical"}]
            }

            tool = GenerateTests(
                audit_report=json.dumps(audit_report),
                target_file=str(empty_file)
            )

            result = tool.run()
            result_data = json.loads(result)

            # Should handle gracefully
            assert result_data["status"] == "success"
            # May generate no tests for empty file
            assert result_data["tests_generated"] >= 0

    def test_malformed_source_code_handling(self):
        """Test handling of syntactically invalid source code."""
        with tempfile.TemporaryDirectory() as temp_dir:
            malformed_file = Path(temp_dir) / "malformed.py"
            malformed_file.write_text('''
def broken_function(
    # Missing closing parenthesis
    return "broken"

class BrokenClass
    # Missing colon
    def method(self):
        pass

invalid syntax here ][
''')

            audit_report = {
                "violations": [{"property": "E2", "severity": "critical"}]
            }

            tool = GenerateTests(
                audit_report=json.dumps(audit_report),
                target_file=str(malformed_file)
            )

            result = tool.run()
            result_data = json.loads(result)

            # Should handle gracefully without crashing
            assert result_data["status"] == "success"
            # May generate no tests due to parse errors
            assert "tests_generated" in result_data

    def test_complex_inheritance_hierarchies(self):
        """Test test generation for complex class hierarchies."""
        with tempfile.TemporaryDirectory() as temp_dir:
            complex_file = Path(temp_dir) / "complex_inheritance.py"
            complex_file.write_text('''
class BaseClass:
    """Base class."""

    def base_method(self):
        """Base method."""
        return "base"

class MiddleClass(BaseClass):
    """Middle class in hierarchy."""

    def middle_method(self):
        """Middle method."""
        return "middle"

    def base_method(self):
        """Override base method."""
        return "overridden"

class ComplexClass(MiddleClass):
    """Complex class with multiple inheritance."""

    def __init__(self, value):
        self.value = value

    def complex_method(self, a, b, c=None):
        """Complex method with multiple parameters."""
        if c is None:
            return a + b + self.value
        return a + b + c + self.value

    @property
    def computed_value(self):
        """Property method."""
        return self.value * 2

    @staticmethod
    def static_method():
        """Static method."""
        return "static"

    @classmethod
    def class_method(cls):
        """Class method."""
        return cls.__name__
''')

            audit_report = {
                "violations": [
                    {"property": "C", "severity": "high", "description": "Comprehensive coverage needed"},
                    {"property": "S", "severity": "medium", "description": "State validation needed"}
                ]
            }

            tool = GenerateTests(
                audit_report=json.dumps(audit_report),
                target_file=str(complex_file)
            )

            result = tool.run()
            result_data = json.loads(result)

            assert result_data["status"] == "success"
            assert result_data["tests_generated"] > 0

            # Verify tests were generated for complex methods
            test_file_path = result_data["test_file"]
            with open(test_file_path, 'r') as f:
                test_content = f.read()

            # Should handle complex inheritance
            assert "ComplexClass" in test_content
            # Should generate tests for methods with multiple parameters
            assert "complex_method" in test_content.lower()

    def test_async_function_test_generation(self):
        """Test generation of tests for async functions."""
        with tempfile.TemporaryDirectory() as temp_dir:
            async_file = Path(temp_dir) / "async_functions.py"
            async_file.write_text('''
import asyncio

async def simple_async_function():
    """Simple async function."""
    await asyncio.sleep(0.1)
    return "async result"

async def async_with_params(param1, param2):
    """Async function with parameters."""
    await asyncio.sleep(0.1)
    return param1 + param2

class AsyncClass:
    """Class with async methods."""

    async def async_method(self):
        """Async method."""
        await asyncio.sleep(0.1)
        return "async method result"

    async def async_method_with_params(self, a, b):
        """Async method with parameters."""
        result = a + b
        await asyncio.sleep(0.1)
        return result
''')

            audit_report = {
                "violations": [
                    {"property": "A", "severity": "critical", "description": "Async testing needed"}
                ]
            }

            tool = GenerateTests(
                audit_report=json.dumps(audit_report),
                target_file=str(async_file)
            )

            result = tool.run()
            result_data = json.loads(result)

            assert result_data["status"] == "success"
            assert result_data["tests_generated"] > 0

            # Verify async tests were generated
            test_file_path = result_data["test_file"]
            with open(test_file_path, 'r') as f:
                test_content = f.read()

            # Should include async test markers and await keywords
            assert "@pytest.mark.asyncio" in test_content
            assert "await" in test_content
            assert "async def test_" in test_content

    def test_unicode_and_special_characters_in_functions(self):
        """Test test generation for functions with unicode characters."""
        with tempfile.TemporaryDirectory() as temp_dir:
            unicode_file = Path(temp_dir) / "unicode_functions.py"
            unicode_file.write_text('''
def Ñ„ÑƒÐ½ÐºÑ†Ð¸Ñ_Ñ_unicode(Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€):
    """Ð¤ÑƒÐ½ÐºÑ†Ð¸Ñ Ñ unicode Ð¸Ð¼ÐµÐ½ÐµÐ¼."""
    return f"Ð ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚: {Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€} ðŸŽ‰"

def function_with_emoji_in_docstring():
    """Function with emoji ðŸš€ in docstring."""
    return "rocket"

class ÐšÐ»Ð°ÑÑÐ¡Ð ÑƒÑÑÐºÐ¸Ð¼Ð˜Ð¼ÐµÐ½ÐµÐ¼:
    """ÐšÐ»Ð°ÑÑ Ñ Ñ€ÑƒÑÑÐºÐ¸Ð¼ Ð¸Ð¼ÐµÐ½ÐµÐ¼."""

    def Ð¼ÐµÑ‚Ð¾Ð´_Ñ_emoji(self, Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ðµ):
        """ÐœÐµÑ‚Ð¾Ð´ Ñ emoji ðŸ’«."""
        return f"âœ¨ {Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ðµ} âœ¨"

def special_chars_123_!@#(param_with_underscores):
    """Function with special characters in name."""
    return param_with_underscores
''', encoding='utf-8')

            audit_report = {
                "violations": [{"property": "N", "severity": "high"}]
            }

            tool = GenerateTests(
                audit_report=json.dumps(audit_report),
                target_file=str(unicode_file)
            )

            result = tool.run()
            result_data = json.loads(result)

            # Should handle unicode gracefully
            assert result_data["status"] == "success"
            assert result_data["tests_generated"] > 0

    def test_deeply_nested_function_structures(self):
        """Test test generation for deeply nested functions."""
        with tempfile.TemporaryDirectory() as temp_dir:
            nested_file = Path(temp_dir) / "nested_functions.py"
            nested_file.write_text('''
def outer_function():
    """Outer function with nested structures."""

    def inner_function():
        """Inner function."""

        def deeply_nested():
            """Deeply nested function."""
            return "deep"

        return deeply_nested()

    class InnerClass:
        """Inner class."""

        def inner_method(self):
            """Inner method."""
            return "inner method"

    return inner_function()

def complex_lambda_function():
    """Function with complex lambda expressions."""
    transform = lambda x: (lambda y: x + y)
    return transform(5)(10)

def function_with_generators():
    """Function that returns generator."""
    return (x ** 2 for x in range(10) if x % 2 == 0)
''')

            audit_report = {
                "violations": [
                    {"property": "E", "severity": "high", "description": "Edge cases for nested structures"}
                ]
            }

            tool = GenerateTests(
                audit_report=json.dumps(audit_report),
                target_file=str(nested_file)
            )

            result = tool.run()
            result_data = json.loads(result)

            assert result_data["status"] == "success"
            # Should detect and generate tests for top-level functions
            assert result_data["tests_generated"] > 0

    def test_massive_function_count_handling(self):
        """Test handling of files with many functions."""
        with tempfile.TemporaryDirectory() as temp_dir:
            massive_file = Path(temp_dir) / "massive_functions.py"

            # Generate file with many functions
            functions = []
            for i in range(50):  # Create 50 functions
                functions.append(f'''
def function_{i}(param_{i}_a, param_{i}_b):
    """Function number {i}."""
    return param_{i}_a + param_{i}_b + {i}
''')

            massive_file.write_text('\n'.join(functions))

            audit_report = {
                "violations": [{"property": "C", "severity": "critical"}]
            }

            tool = GenerateTests(
                audit_report=json.dumps(audit_report),
                target_file=str(massive_file)
            )

            # Should complete in reasonable time
            start_time = time.time()
            result = tool.run()
            end_time = time.time()

            assert (end_time - start_time) < 30  # Should complete within 30 seconds

            result_data = json.loads(result)
            assert result_data["status"] == "success"
            assert result_data["tests_generated"] > 0


class TestTestGeneratorErrorConditions:
    """Error condition testing for test generator."""

    def test_invalid_audit_report_handling(self):
        """Test handling of invalid audit reports."""
        invalid_reports = [
            "not json at all",
            '{"invalid": "structure"}',
            '{"violations": "not a list"}',
            '{"violations": [{"invalid": "violation structure"}]}',
            "",
            "null",
            '{"violations": []}',  # Valid but empty
        ]

        with tempfile.TemporaryDirectory() as temp_dir:
            test_file = Path(temp_dir) / "test.py"
            test_file.write_text("def test_func(): pass")

            for invalid_report in invalid_reports:
                tool = GenerateTests(
                    audit_report=invalid_report,
                    target_file=str(test_file)
                )

                result = tool.run()
                result_data = json.loads(result)

                # Should handle gracefully
                if invalid_report in ["not json at all", "", "null"]:
                    assert "error" in result_data
                else:
                    # May succeed with empty/minimal reports
                    assert "status" in result_data or "error" in result_data

    def test_permission_denied_scenarios(self):
        """Test handling of permission-denied file access."""
        if os.name == 'posix':  # Unix-like systems
            # Try to generate tests for a system file
            restricted_file = "/etc/passwd"

            audit_report = {
                "violations": [{"property": "N", "severity": "critical"}]
            }

            tool = GenerateTests(
                audit_report=json.dumps(audit_report),
                target_file=restricted_file
            )

            result = tool.run()
            result_data = json.loads(result)

            # Should handle permission errors gracefully
            assert "error" in result_data or result_data.get("status") == "success"

    def test_disk_full_simulation(self):
        """Test behavior when disk is full (simulated)."""
        with tempfile.TemporaryDirectory() as temp_dir:
            test_file = Path(temp_dir) / "test.py"
            test_file.write_text("def test_func(): pass")

            audit_report = {
                "violations": [{"property": "N", "severity": "critical"}]
            }

            tool = GenerateTests(
                audit_report=json.dumps(audit_report),
                target_file=str(test_file)
            )

            # Mock file writing to simulate disk full error
            with patch('builtins.open', mock_open()) as mock_file:
                mock_file.side_effect = OSError("No space left on device")

                result = tool.run()
                result_data = json.loads(result)

                # Should handle disk full error gracefully
                # May succeed if it doesn't reach the file writing stage
                assert isinstance(result_data, dict)

    def test_concurrent_file_modification(self):
        """Test behavior when source file is modified during analysis."""
        with tempfile.TemporaryDirectory() as temp_dir:
            source_file = Path(temp_dir) / "concurrent.py"
            source_file.write_text("def original_func(): pass")

            audit_report = {
                "violations": [{"property": "N", "severity": "critical"}]
            }

            tool = GenerateTests(
                audit_report=json.dumps(audit_report),
                target_file=str(source_file)
            )

            # Mock source analysis to modify file during analysis
            original_analyze = tool._analyze_source_file

            def mock_analyze(file_path):
                # Modify file during analysis
                Path(file_path).write_text("def modified_func(): pass")
                return original_analyze(file_path)

            with patch.object(tool, '_analyze_source_file', side_effect=mock_analyze):
                result = tool.run()
                result_data = json.loads(result)

                # Should handle gracefully
                assert "status" in result_data

    def test_memory_pressure_handling(self):
        """Test behavior under memory pressure (simulated)."""
        with tempfile.TemporaryDirectory() as temp_dir:
            # Create a complex file that would require significant memory
            complex_file = Path(temp_dir) / "memory_intensive.py"

            # Generate complex code structure
            code_parts = []
            for i in range(100):
                code_parts.append(f'''
class MemoryIntensiveClass_{i}:
    """Memory intensive class {i}."""

    def __init__(self):
        self.data_{i} = [{j} for j in range(100)]

    def method_{i}(self, param_a, param_b, param_c, param_d, param_e):
        """Method with many parameters."""
        return sum([param_a, param_b, param_c, param_d, param_e, i])
''')

            complex_file.write_text('\n'.join(code_parts))

            audit_report = {
                "violations": [{"property": "C", "severity": "critical"}]
            }

            tool = GenerateTests(
                audit_report=json.dumps(audit_report),
                target_file=str(complex_file)
            )

            # Should complete within reasonable time and memory
            start_time = time.time()
            result = tool.run()
            end_time = time.time()

            assert (end_time - start_time) < 60  # Should complete within 1 minute

            result_data = json.loads(result)
            assert result_data["status"] == "success"


class TestTestGeneratorQualityValidation:
    """Test quality validation of generated tests."""

    def test_generated_test_syntax_validity(self):
        """Test that generated tests have valid Python syntax."""
        with tempfile.TemporaryDirectory() as temp_dir:
            source_file = Path(temp_dir) / "syntax_test.py"
            source_file.write_text('''
def simple_function(a, b):
    """Simple function for syntax testing."""
    return a + b

class SimpleClass:
    """Simple class for syntax testing."""

    def method(self, x):
        """Simple method."""
        return x * 2
''')

            audit_report = {
                "violations": [{"property": "N", "severity": "critical"}]
            }

            tool = GenerateTests(
                audit_report=json.dumps(audit_report),
                target_file=str(source_file)
            )

            result = tool.run()
            result_data = json.loads(result)

            assert result_data["status"] == "success"

            # Read generated test file and validate syntax
            test_file_path = result_data["test_file"]
            with open(test_file_path, 'r') as f:
                test_content = f.read()

            # Should be valid Python syntax
            try:
                ast.parse(test_content)
                syntax_valid = True
            except SyntaxError:
                syntax_valid = False

            assert syntax_valid, "Generated test code should have valid syntax"

    def test_generated_test_completeness(self):
        """Test that generated tests are complete and meaningful."""
        with tempfile.TemporaryDirectory() as temp_dir:
            source_file = Path(temp_dir) / "completeness_test.py"
            source_file.write_text('''
def add_numbers(a, b):
    """Add two numbers."""
    return a + b

def divide_safely(a, b):
    """Divide with error handling."""
    if b == 0:
        raise ValueError("Cannot divide by zero")
    return a / b
''')

            audit_report = {
                "violations": [
                    {"property": "N", "severity": "critical"},
                    {"property": "E2", "severity": "critical"}
                ]
            }

            tool = GenerateTests(
                audit_report=json.dumps(audit_report),
                target_file=str(source_file)
            )

            result = tool.run()
            result_data = json.loads(result)

            test_file_path = result_data["test_file"]
            with open(test_file_path, 'r') as f:
                test_content = f.read()

            # Should include proper test structure
            assert 'def test_' in test_content
            assert 'import pytest' in test_content or 'import' in test_content
            assert 'assert' in test_content

            # For error condition violation, should include error testing
            if any('E2' in str(v) for v in audit_report["violations"]):
                assert 'pytest.raises' in test_content

    def test_aaa_pattern_compliance(self):
        """Test that generated tests follow Arrange-Act-Assert pattern."""
        with tempfile.TemporaryDirectory() as temp_dir:
            source_file = Path(temp_dir) / "aaa_test.py"
            source_file.write_text('''
def calculate_area(length, width):
    """Calculate rectangle area."""
    return length * width
''')

            audit_report = {
                "violations": [{"property": "N", "severity": "critical"}]
            }

            tool = GenerateTests(
                audit_report=json.dumps(audit_report),
                target_file=str(source_file)
            )

            result = tool.run()
            result_data = json.loads(result)

            test_file_path = result_data["test_file"]
            with open(test_file_path, 'r') as f:
                test_content = f.read()

            # Check for AAA pattern (though not all generated tests may include comments)
            # At minimum, should have logical structure
            assert 'def test_' in test_content
            assert 'assert' in test_content

    def test_import_statement_correctness(self):
        """Test that generated tests have correct import statements."""
        with tempfile.TemporaryDirectory() as temp_dir:
            source_file = Path(temp_dir) / "import_test.py"
            source_file.write_text('''
def utility_function():
    """Utility function for import testing."""
    return "utility"

class UtilityClass:
    """Utility class for import testing."""

    def method(self):
        """Utility method."""
        return "method"
''')

            audit_report = {
                "violations": [{"property": "N", "severity": "critical"}]
            }

            tool = GenerateTests(
                audit_report=json.dumps(audit_report),
                target_file=str(source_file)
            )

            result = tool.run()
            result_data = json.loads(result)

            test_file_path = result_data["test_file"]
            with open(test_file_path, 'r') as f:
                test_content = f.read()

            # Should have appropriate import statements
            module_name = Path(source_file).stem
            assert f'from {module_name} import' in test_content

    def test_test_naming_conventions(self):
        """Test that generated tests follow proper naming conventions."""
        with tempfile.TemporaryDirectory() as temp_dir:
            source_file = Path(temp_dir) / "naming_test.py"
            source_file.write_text('''
def complex_calculation(x, y, z):
    """Complex calculation function."""
    return x * y + z

class DataProcessor:
    """Data processor class."""

    def process_data(self, data):
        """Process data method."""
        return [item * 2 for item in data]
''')

            audit_report = {
                "violations": [{"property": "N", "severity": "critical"}]
            }

            tool = GenerateTests(
                audit_report=json.dumps(audit_report),
                target_file=str(source_file)
            )

            result = tool.run()
            result_data = json.loads(result)

            # Check test names in result
            test_names = result_data.get("test_names", [])

            for name in test_names:
                # Should start with test_
                assert name.startswith("test_"), f"Test name '{name}' should start with 'test_'"
                # Should be descriptive
                assert len(name) > 5, f"Test name '{name}' should be descriptive"