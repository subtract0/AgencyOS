"""
End-to-end integration tests for the complete CodeHealer framework cycle.
Tests the audit → generate → verify healing workflow.

This file addresses the critical gap in integration testing by:
1. Testing complete audit-to-test-generation workflows
2. Verifying the healing process actually improves Q(T) scores
3. Testing multi-agent coordination and communication
4. Validating the philosophical completeness of self-healing
5. Performance testing of the complete pipeline

Generated by TestGeneratorAgent for Q(T) healing: Integration Testing
"""

import json
import os
import tempfile
import time
from pathlib import Path

import pytest

from auditor_agent.auditor_agent import AnalyzeCodebase
from test_generator_agent.test_generator_agent import GenerateTests


class TestCodeHealerEndToEndWorkflow:
    """Complete end-to-end workflow testing for the CodeHealer framework."""

    @pytest.fixture
    def unhealthy_codebase(self):
        """Create a deliberately unhealthy codebase for healing testing."""
        temp_dir = tempfile.mkdtemp()

        # Create source file with poor test coverage
        source_file = Path(temp_dir) / "calculator.py"
        source_file.write_text('''
"""
Calculator module with various functions.
This module intentionally has poor test coverage for healing demonstration.
"""

def add(a, b):
    """Add two numbers."""
    return a + b

def subtract(a, b):
    """Subtract second number from first."""
    return a - b

def multiply(a, b):
    """Multiply two numbers."""
    if a == 0 or b == 0:
        return 0
    return a * b

def divide(a, b):
    """Divide first number by second."""
    if b == 0:
        raise ValueError("Cannot divide by zero")
    return a / b

def power(base, exponent):
    """Calculate base raised to exponent."""
    if exponent == 0:
        return 1
    elif exponent < 0:
        return 1 / power(base, -exponent)
    else:
        result = 1
        for _ in range(exponent):
            result *= base
        return result

class Calculator:
    """Calculator class with state."""

    def __init__(self):
        self.history = []
        self.memory = 0

    def calculate(self, operation, a, b):
        """Perform calculation and store in history."""
        if operation == "add":
            result = add(a, b)
        elif operation == "subtract":
            result = subtract(a, b)
        elif operation == "multiply":
            result = multiply(a, b)
        elif operation == "divide":
            result = divide(a, b)
        else:
            raise ValueError(f"Unknown operation: {operation}")

        self.history.append(f"{a} {operation} {b} = {result}")
        return result

    def store_in_memory(self, value):
        """Store value in memory."""
        self.memory = value

    def recall_memory(self):
        """Recall value from memory."""
        return self.memory

    def clear_history(self):
        """Clear calculation history."""
        self.history = []

    async def async_calculate(self, operation, a, b):
        """Async version of calculate."""
        await asyncio.sleep(0.01)  # Simulate async work
        return self.calculate(operation, a, b)

    def get_statistics(self):
        """Get statistics about calculations."""
        if not self.history:
            return {"count": 0, "operations": []}

        operations = []
        for entry in self.history:
            if "add" in entry:
                operations.append("add")
            elif "subtract" in entry:
                operations.append("subtract")
            elif "multiply" in entry:
                operations.append("multiply")
            elif "divide" in entry:
                operations.append("divide")

        return {
            "count": len(self.history),
            "operations": operations,
            "unique_operations": list(set(operations))
        }
''')

        # Create minimal, inadequate test file
        test_file = Path(temp_dir) / "test_calculator.py"
        test_file.write_text('''
"""
Minimal test file for calculator.
Intentionally insufficient for demonstration of healing.
"""

def test_add_basic():
    """Test basic addition."""
    from calculator import add
    assert add(2, 3) == 5

def test_calculator_init():
    """Test calculator initialization."""
    from calculator import Calculator
    calc = Calculator()
    assert calc.history == []
''')

        yield temp_dir

        import shutil

        shutil.rmtree(temp_dir, ignore_errors=True)

    def test_complete_healing_workflow(self, unhealthy_codebase):
        """Test the complete audit → generate → verify healing cycle."""
        # Phase 1: Initial Audit
        print("Phase 1: Initial audit of unhealthy codebase")
        audit_tool = AnalyzeCodebase(target_path=unhealthy_codebase, mode="full")
        initial_audit_result = audit_tool.run()
        initial_audit_data = json.loads(initial_audit_result)

        # Verify initial poor health
        initial_qt_score = initial_audit_data["qt_score"]
        assert initial_qt_score < 0.7, f"Initial Q(T) should be poor: {initial_qt_score}"

        print(f"Initial Q(T) score: {initial_qt_score}")
        print(f"Initial violations: {len(initial_audit_data['violations'])}")

        # Phase 2: Test Generation
        print("Phase 2: Generating healing tests")
        source_file = Path(unhealthy_codebase) / "calculator.py"
        generate_tool = GenerateTests(
            audit_report=initial_audit_result, target_file=str(source_file)
        )

        generation_result = generate_tool.run()
        generation_data = json.loads(generation_result)

        assert generation_data["status"] == "success"
        assert generation_data["tests_generated"] > 0
        assert generation_data["violations_addressed"] > 0

        print(f"Generated {generation_data['tests_generated']} tests")
        print(f"Addressed {generation_data['violations_addressed']} violations")

        # Phase 3: Verification Audit
        print("Phase 3: Re-auditing after test generation")
        verification_tool = AnalyzeCodebase(target_path=unhealthy_codebase, mode="verification")
        final_audit_result = verification_tool.run()
        final_audit_data = json.loads(final_audit_result)

        final_qt_score = final_audit_data["qt_score"]
        print(f"Final Q(T) score: {final_qt_score}")
        print(f"Final violations: {len(final_audit_data['violations'])}")

        # Phase 4: Healing Verification
        print("Phase 4: Verifying healing effectiveness")

        # Q(T) score should at least not get worse (improvement might be minimal in test)
        improvement = final_qt_score - initial_qt_score
        assert improvement >= 0, f"Q(T) should not worsen: {initial_qt_score} → {final_qt_score}"

        # Should have fewer critical violations
        initial_critical = len(
            [v for v in initial_audit_data["violations"] if v["severity"] == "critical"]
        )
        final_critical = len(
            [v for v in final_audit_data["violations"] if v["severity"] == "critical"]
        )

        print(f"Critical violations: {initial_critical} → {final_critical}")

        # Verify generated test file exists and is substantial
        generated_test_file = generation_data["test_file"]
        assert os.path.exists(generated_test_file)

        with open(generated_test_file) as f:
            test_content = f.read()

        assert len(test_content) > 1000, "Generated tests should be substantial"
        assert "def test_" in test_content
        assert "assert" in test_content

        # Store results in instance variable for inspection if needed
        self.healing_results = {
            "initial_qt": initial_qt_score,
            "final_qt": final_qt_score,
            "improvement": improvement,
            "tests_generated": generation_data["tests_generated"],
            "violations_addressed": generation_data["violations_addressed"],
        }

    def test_iterative_healing_process(self, unhealthy_codebase):
        """Test multiple iterations of the healing process."""
        current_codebase = unhealthy_codebase
        qt_scores = []
        iterations = 3

        for iteration in range(iterations):
            print(f"Healing iteration {iteration + 1}")

            # Audit
            audit_tool = AnalyzeCodebase(target_path=current_codebase, mode="full")
            audit_result = audit_tool.run()
            audit_data = json.loads(audit_result)

            qt_score = audit_data["qt_score"]
            qt_scores.append(qt_score)
            print(f"Iteration {iteration + 1} Q(T): {qt_score}")

            # If we have high-severity violations, generate more tests
            high_severity_violations = [
                v for v in audit_data["violations"] if v["severity"] in ["critical", "high"]
            ]

            if high_severity_violations:
                # Generate tests for remaining violations
                source_file = Path(current_codebase) / "calculator.py"
                generate_tool = GenerateTests(
                    audit_report=audit_result, target_file=str(source_file)
                )

                generation_result = generate_tool.run()
                generation_data = json.loads(generation_result)

                print(
                    f"Iteration {iteration + 1}: Generated {generation_data.get('tests_generated', 0)} tests"
                )

        # Verify progressive improvement
        for i in range(1, len(qt_scores)):
            # Allow for some fluctuation but overall trend should be positive
            if i == len(qt_scores) - 1:  # Last iteration should definitely be better
                assert qt_scores[i] >= qt_scores[0], (
                    f"Final Q(T) should be better than initial: {qt_scores}"
                )

        print(f"Q(T) progression: {qt_scores}")

    def test_multi_file_codebase_healing(self):
        """Test healing of a multi-file codebase."""
        with tempfile.TemporaryDirectory() as temp_dir:
            # Create multiple source files
            math_utils = Path(temp_dir) / "math_utils.py"
            math_utils.write_text('''
def factorial(n):
    """Calculate factorial."""
    if n <= 1:
        return 1
    return n * factorial(n - 1)

def fibonacci(n):
    """Calculate nth Fibonacci number."""
    if n <= 1:
        return n
    return fibonacci(n - 1) + fibonacci(n - 2)
''')

            string_utils = Path(temp_dir) / "string_utils.py"
            string_utils.write_text('''
def reverse_string(s):
    """Reverse a string."""
    return s[::-1]

def count_words(text):
    """Count words in text."""
    if not text:
        return 0
    return len(text.split())

def capitalize_words(text):
    """Capitalize each word."""
    return ' '.join(word.capitalize() for word in text.split())
''')

            # Create minimal tests
            test_math = Path(temp_dir) / "test_math_utils.py"
            test_math.write_text("""
def test_factorial_basic():
    from math_utils import factorial
    assert factorial(5) == 120
""")

            # Initial audit
            audit_tool = AnalyzeCodebase(target_path=temp_dir, mode="full")
            initial_result = audit_tool.run()
            initial_data = json.loads(initial_result)

            initial_qt = initial_data["qt_score"]
            print(f"Multi-file initial Q(T): {initial_qt}")

            # Generate tests for each file
            for source_file in [math_utils, string_utils]:
                generate_tool = GenerateTests(
                    audit_report=initial_result, target_file=str(source_file)
                )

                generation_result = generate_tool.run()
                generation_data = json.loads(generation_result)

                assert generation_data["status"] == "success"
                print(
                    f"Generated tests for {source_file.name}: {generation_data['tests_generated']}"
                )

            # Final audit
            final_audit_tool = AnalyzeCodebase(target_path=temp_dir, mode="verification")
            final_result = final_audit_tool.run()
            final_data = json.loads(final_result)

            final_qt = final_data["qt_score"]
            print(f"Multi-file final Q(T): {final_qt}")

            # Should show improvement
            assert final_qt > initial_qt

    def test_edge_case_healing_robustness(self):
        """Test healing process robustness with edge cases."""
        edge_cases = [
            # Empty file
            ("empty.py", ""),
            # Only comments
            (
                "comments_only.py",
                """
# This file only has comments
# No actual code to test
# Should handle gracefully
""",
            ),
            # Single function
            (
                "single_function.py",
                '''
def hello():
    """Single function file."""
    return "hello"
''',
            ),
            # Complex edge case
            (
                "complex_edge.py",
                '''
def edge_case_function(param=None):
    """Function with default parameters and edge cases."""
    if param is None:
        return []

    if isinstance(param, str):
        if not param:
            return ["empty"]
        return param.split()

    if isinstance(param, (int, float)):
        if param == 0:
            return [0]
        elif param < 0:
            return list(range(param, 0))
        else:
            return list(range(param))

    return [str(param)]
''',
            ),
        ]

        for filename, content in edge_cases:
            with tempfile.TemporaryDirectory() as temp_dir:
                source_file = Path(temp_dir) / filename
                source_file.write_text(content)

                # Audit
                audit_tool = AnalyzeCodebase(target_path=temp_dir)
                audit_result = audit_tool.run()
                audit_data = json.loads(audit_result)

                print(f"Edge case {filename} Q(T): {audit_data['qt_score']}")

                # Generate tests
                if audit_data["codebase_analysis"]["total_behaviors"] > 0:
                    generate_tool = GenerateTests(
                        audit_report=audit_result, target_file=str(source_file)
                    )

                    generation_result = generate_tool.run()
                    generation_data = json.loads(generation_result)

                    # Should handle gracefully
                    assert generation_data["status"] == "success"
                    print(
                        f"Edge case {filename}: Generated {generation_data['tests_generated']} tests"
                    )


class TestCodeHealerPerformanceIntegration:
    """Performance testing for the complete healing pipeline."""

    def test_large_codebase_healing_performance(self):
        """Test healing performance on larger codebases."""
        with tempfile.TemporaryDirectory() as temp_dir:
            # Create a larger codebase
            num_files = 10
            functions_per_file = 20

            for file_idx in range(num_files):
                file_path = Path(temp_dir) / f"module_{file_idx}.py"
                functions = []

                for func_idx in range(functions_per_file):
                    functions.append(f'''
def function_{file_idx}_{func_idx}(param_a, param_b=None):
    """Function {func_idx} in module {file_idx}."""
    if param_b is None:
        param_b = param_a

    if param_a == 0:
        return param_b

    return param_a + param_b + {func_idx}

class Class_{file_idx}_{func_idx}:
    """Class {func_idx} in module {file_idx}."""

    def __init__(self, value={func_idx}):
        self.value = value

    def method_{func_idx}(self):
        """Method {func_idx}."""
        return self.value * {func_idx + 1}
''')

                file_path.write_text("\n".join(functions))

            # Create some basic tests
            test_file = Path(temp_dir) / "test_basic.py"
            test_file.write_text('''
def test_basic():
    """Basic test."""
    assert True
''')

            # Time the complete healing process
            start_time = time.time()

            # Audit
            audit_tool = AnalyzeCodebase(target_path=temp_dir)
            audit_result = audit_tool.run()
            audit_data = json.loads(audit_result)

            audit_time = time.time() - start_time
            print(f"Large codebase audit time: {audit_time:.2f}s")
            print(f"Behaviors found: {audit_data['codebase_analysis']['total_behaviors']}")

            # Generate tests for first few files only (to keep test time reasonable)
            generation_start = time.time()

            for file_idx in range(min(3, num_files)):  # Test first 3 files
                source_file = Path(temp_dir) / f"module_{file_idx}.py"
                generate_tool = GenerateTests(
                    audit_report=audit_result, target_file=str(source_file)
                )

                generation_result = generate_tool.run()
                generation_data = json.loads(generation_result)

                assert generation_data["status"] == "success"

            generation_time = time.time() - generation_start
            print(f"Test generation time: {generation_time:.2f}s")

            total_time = time.time() - start_time
            print(f"Total healing time: {total_time:.2f}s")

            # Performance should be reasonable
            assert audit_time < 30, f"Audit should complete in reasonable time: {audit_time}s"
            assert generation_time < 60, (
                f"Generation should complete in reasonable time: {generation_time}s"
            )

    def test_concurrent_healing_safety(self):
        """Test safety of concurrent healing operations."""
        import queue
        import threading

        with tempfile.TemporaryDirectory() as temp_dir:
            # Create multiple small codebases
            for i in range(3):
                subdir = Path(temp_dir) / f"codebase_{i}"
                subdir.mkdir()

                source_file = subdir / f"module_{i}.py"
                source_file.write_text(f'''
def function_{i}():
    """Function {i}."""
    return {i}

class Class_{i}:
    """Class {i}."""

    def method_{i}(self):
        """Method {i}."""
        return {i} * 2
''')

            results_queue = queue.Queue()

            def healing_worker(codebase_path):
                """Worker function for concurrent healing."""
                try:
                    # Audit
                    audit_tool = AnalyzeCodebase(target_path=str(codebase_path))
                    audit_result = audit_tool.run()
                    audit_data = json.loads(audit_result)

                    # Generate tests
                    source_files = list(codebase_path.glob("*.py"))
                    if source_files:
                        generate_tool = GenerateTests(
                            audit_report=audit_result, target_file=str(source_files[0])
                        )

                        generation_result = generate_tool.run()
                        generation_data = json.loads(generation_result)

                        results_queue.put(
                            {
                                "codebase": str(codebase_path),
                                "status": "success",
                                "qt_score": audit_data["qt_score"],
                                "tests_generated": generation_data["tests_generated"],
                            }
                        )
                    else:
                        results_queue.put({"codebase": str(codebase_path), "status": "no_files"})

                except Exception as e:
                    results_queue.put(
                        {"codebase": str(codebase_path), "status": "error", "error": str(e)}
                    )

            # Start concurrent healing operations
            threads = []
            for i in range(3):
                codebase_path = Path(temp_dir) / f"codebase_{i}"
                thread = threading.Thread(target=healing_worker, args=(codebase_path,))
                threads.append(thread)
                thread.start()

            # Wait for completion
            for thread in threads:
                thread.join(timeout=30)
                assert not thread.is_alive(), "Healing thread should complete"

            # Verify all completed successfully
            results = []
            while not results_queue.empty():
                results.append(results_queue.get())

            assert len(results) == 3
            for result in results:
                assert result["status"] in ["success", "no_files"], (
                    f"Healing should succeed: {result}"
                )

    def test_memory_usage_during_healing(self):
        """Test memory usage stays reasonable during healing."""
        import gc

        with tempfile.TemporaryDirectory() as temp_dir:
            # Create a moderately complex codebase
            for i in range(5):
                source_file = Path(temp_dir) / f"memory_test_{i}.py"
                functions = []

                for j in range(30):
                    functions.append(f'''
def memory_function_{i}_{j}(data):
    """Memory function {i}_{j}."""
    if not data:
        return []

    result = []
    for item in data:
        if isinstance(item, str):
            result.append(item.upper())
        elif isinstance(item, (int, float)):
            result.append(item * {j + 1})
        else:
            result.append(str(item))

    return result
''')

                source_file.write_text("\n".join(functions))

            # Measure initial memory
            gc.collect()
            initial_objects = len(gc.get_objects())

            # Perform healing
            audit_tool = AnalyzeCodebase(target_path=temp_dir)
            audit_result = audit_tool.run()

            # Generate tests for a couple of files
            for i in range(2):
                source_file = Path(temp_dir) / f"memory_test_{i}.py"
                generate_tool = GenerateTests(
                    audit_report=audit_result, target_file=str(source_file)
                )

                generation_result = generate_tool.run()
                generation_data = json.loads(generation_result)

                assert generation_data["status"] == "success"

                # Clear references
                del generate_tool

            # Measure final memory
            gc.collect()
            final_objects = len(gc.get_objects())

            memory_growth = final_objects - initial_objects
            print(f"Memory growth during healing: {memory_growth} objects")

            # Memory growth should be reasonable
            assert memory_growth < 5000, f"Memory growth should be reasonable: {memory_growth}"


class TestCodeHealerPhilosophicalCompleteness:
    """Test the philosophical aspects of self-healing and meta-testing."""

    def test_framework_heals_itself(self):
        """Test that the framework can heal its own components."""
        # This test uses the actual auditor and test generator files
        # to test if they can generate healing tests for themselves

        auditor_file = Path(__file__).parent.parent / "auditor_agent" / "auditor_agent.py"
        generator_file = (
            Path(__file__).parent.parent / "test_generator_agent" / "test_generator_agent.py"
        )

        # Test healing the auditor
        if auditor_file.exists():
            audit_tool = AnalyzeCodebase(target_path=str(auditor_file.parent))
            audit_result = audit_tool.run()
            audit_data = json.loads(audit_result)

            print(f"Auditor self-audit Q(T): {audit_data['qt_score']}")

            # Try to generate tests for the auditor
            generate_tool = GenerateTests(audit_report=audit_result, target_file=str(auditor_file))

            with tempfile.TemporaryDirectory() as temp_dir:
                # Modify tool to write to temp location for testing

                def mock_get_test_file_path(source_file):
                    return str(Path(temp_dir) / "test_auditor_self_heal.py")

                generate_tool._get_test_file_path = mock_get_test_file_path

                generation_result = generate_tool.run()
                generation_data = json.loads(generation_result)

                # Should succeed in generating tests for itself
                assert generation_data["status"] == "success"
                print(f"Self-healing auditor: {generation_data['tests_generated']} tests generated")

        # Test healing the test generator
        if generator_file.exists():
            audit_tool = AnalyzeCodebase(target_path=str(generator_file.parent))
            audit_result = audit_tool.run()
            audit_data = json.loads(audit_result)

            print(f"Generator self-audit Q(T): {audit_data['qt_score']}")

            generate_tool = GenerateTests(
                audit_report=audit_result, target_file=str(generator_file)
            )

            with tempfile.TemporaryDirectory() as temp_dir:

                def mock_get_test_file_path(source_file):
                    return str(Path(temp_dir) / "test_generator_self_heal.py")

                generate_tool._get_test_file_path = mock_get_test_file_path

                generation_result = generate_tool.run()
                generation_data = json.loads(generation_result)

                assert generation_data["status"] == "success"
                print(
                    f"Self-healing generator: {generation_data['tests_generated']} tests generated"
                )

    def test_infinite_recursion_prevention_in_practice(self):
        """Test that self-healing doesn't create infinite recursion."""
        with tempfile.TemporaryDirectory() as temp_dir:
            # Create a self-referential test generator
            self_generator = Path(temp_dir) / "self_generator.py"
            self_generator.write_text('''
def generate_test_for_self():
    """Generate test for this very function."""
    return """
def test_generate_test_for_self():
    from self_generator import generate_test_for_self
    result = generate_test_for_self()
    assert "def test_" in result
"""

def analyze_and_generate(target_function):
    """Analyze and generate tests for a function."""
    if target_function == "generate_test_for_self":
        return generate_test_for_self()
    elif target_function == "analyze_and_generate":
        # This could cause infinite recursion
        return analyze_and_generate("analyze_and_generate")
    else:
        return f"def test_{target_function}(): pass"
''')

            # Perform healing - should not hang
            start_time = time.time()

            audit_tool = AnalyzeCodebase(target_path=temp_dir)
            audit_result = audit_tool.run()

            generate_tool = GenerateTests(
                audit_report=audit_result, target_file=str(self_generator)
            )

            generation_result = generate_tool.run()
            generation_data = json.loads(generation_result)

            end_time = time.time()

            # Should complete quickly without infinite recursion
            assert (end_time - start_time) < 10, "Should not hang in infinite recursion"
            assert generation_data["status"] == "success"

    def test_quality_assessment_convergence(self):
        """Test that quality assessment eventually converges."""
        with tempfile.TemporaryDirectory() as temp_dir:
            # Create a codebase that can be incrementally improved
            source_file = Path(temp_dir) / "convergence_test.py"
            source_file.write_text('''
def simple_function():
    """Simple function for convergence testing."""
    return "simple"

def another_function(param):
    """Another function with parameter."""
    return param * 2

class SimpleClass:
    """Simple class for convergence testing."""

    def method(self):
        """Simple method."""
        return "method"
''')

            qt_scores = []
            max_iterations = 5
            convergence_threshold = 0.05  # Consider converged if change < 5%

            for iteration in range(max_iterations):
                # Audit
                audit_tool = AnalyzeCodebase(target_path=temp_dir)
                audit_result = audit_tool.run()
                audit_data = json.loads(audit_result)

                qt_score = audit_data["qt_score"]
                qt_scores.append(qt_score)

                print(f"Convergence iteration {iteration + 1}: Q(T) = {qt_score}")

                # Check for convergence
                if len(qt_scores) >= 2:
                    improvement = qt_scores[-1] - qt_scores[-2]
                    if abs(improvement) < convergence_threshold:
                        print(f"Converged after {iteration + 1} iterations")
                        break

                # Generate more tests
                if audit_data["violations"]:
                    generate_tool = GenerateTests(
                        audit_report=audit_result, target_file=str(source_file)
                    )

                    generation_result = generate_tool.run()
                    generation_data = json.loads(generation_result)

                    if generation_data["tests_generated"] == 0:
                        print("No more tests to generate")
                        break

            # Should show improvement over iterations
            if len(qt_scores) > 1:
                final_improvement = qt_scores[-1] - qt_scores[0]
                print(f"Total Q(T) improvement: {final_improvement}")
                assert final_improvement >= 0, "Q(T) should not decrease over iterations"
