"""
End-to-end integration tests for the complete CodeHealer framework cycle.
Tests the audit → generate → verify healing workflow.

This file addresses the critical gap in integration testing by:
1. Testing complete audit-to-test-generation workflows
2. Verifying the healing process actually improves Q(T) scores
3. Testing multi-agent coordination and communication
4. Validating the philosophical completeness of self-healing
5. Performance testing of the complete pipeline

Generated by TestGeneratorAgent for Q(T) healing: Integration Testing
"""

import os
import json
import tempfile
import asyncio
import time
from pathlib import Path
from unittest.mock import Mock, patch, MagicMock
from typing import Dict, List, Any

import pytest

from auditor_agent.auditor_agent import create_auditor_agent, AnalyzeCodebase
from test_generator_agent.test_generator_agent import create_test_generator_agent, GenerateTests
from shared.agent_context import create_agent_context


class TestCodeHealerEndToEndWorkflow:
    """Complete end-to-end workflow testing for the CodeHealer framework."""

    @pytest.fixture
    def unhealthy_codebase(self):
        """Create a deliberately unhealthy codebase for healing testing."""
        temp_dir = tempfile.mkdtemp()

        # Create source file with poor test coverage
        source_file = Path(temp_dir) / "calculator.py"
        source_file.write_text('''
"""
Calculator module with various functions.
This module intentionally has poor test coverage for healing demonstration.
"""

def add(a, b):
    """Add two numbers."""
    return a + b

def subtract(a, b):
    """Subtract second number from first."""
    return a - b

def multiply(a, b):
    """Multiply two numbers."""
    if a == 0 or b == 0:
        return 0
    return a * b

def divide(a, b):
    """Divide first number by second."""
    if b == 0:
        raise ValueError("Cannot divide by zero")
    return a / b

def power(base, exponent):
    """Calculate base raised to exponent."""
    if exponent == 0:
        return 1
    elif exponent < 0:
        return 1 / power(base, -exponent)
    else:
        result = 1
        for _ in range(exponent):
            result *= base
        return result

class Calculator:
    """Calculator class with state."""

    def __init__(self):
        self.history = []
        self.memory = 0

    def calculate(self, operation, a, b):
        """Perform calculation and store in history."""
        if operation == "add":
            result = add(a, b)
        elif operation == "subtract":
            result = subtract(a, b)
        elif operation == "multiply":
            result = multiply(a, b)
        elif operation == "divide":
            result = divide(a, b)
        else:
            raise ValueError(f"Unknown operation: {operation}")

        self.history.append(f"{a} {operation} {b} = {result}")
        return result

    def store_in_memory(self, value):
        """Store value in memory."""
        self.memory = value

    def recall_memory(self):
        """Recall value from memory."""
        return self.memory

    def clear_history(self):
        """Clear calculation history."""
        self.history = []

    async def async_calculate(self, operation, a, b):
        """Async version of calculate."""
        await asyncio.sleep(0.01)  # Simulate async work
        return self.calculate(operation, a, b)

    def get_statistics(self):
        """Get statistics about calculations."""
        if not self.history:
            return {"count": 0, "operations": []}

        operations = []
        for entry in self.history:
            if "add" in entry:
                operations.append("add")
            elif "subtract" in entry:
                operations.append("subtract")
            elif "multiply" in entry:
                operations.append("multiply")
            elif "divide" in entry:
                operations.append("divide")

        return {
            "count": len(self.history),
            "operations": operations,
            "unique_operations": list(set(operations))
        }
''')

        # Create minimal, inadequate test file
        test_file = Path(temp_dir) / "test_calculator.py"
        test_file.write_text('''
"""
Minimal test file for calculator.
Intentionally insufficient for demonstration of healing.
"""

def test_add_basic():
    """Test basic addition."""
    from calculator import add
    assert add(2, 3) == 5

def test_calculator_init():
    """Test calculator initialization."""
    from calculator import Calculator
    calc = Calculator()
    assert calc.history == []
''')

        yield temp_dir

        import shutil
        shutil.rmtree(temp_dir, ignore_errors=True)

    def test_complete_healing_workflow(self, unhealthy_codebase):
        """Test the complete audit → generate → verify healing cycle."""
        # Phase 1: Initial Audit
        print("Phase 1: Initial audit of unhealthy codebase")
        audit_tool = AnalyzeCodebase(target_path=unhealthy_codebase, mode="full")
        initial_audit_result = audit_tool.run()
        initial_audit_data = json.loads(initial_audit_result)

        # Verify initial poor health
        initial_qt_score = initial_audit_data["qt_score"]
        assert initial_qt_score < 0.7, f"Initial Q(T) should be poor: {initial_qt_score}"

        print(f"Initial Q(T) score: {initial_qt_score}")
        print(f"Initial violations: {len(initial_audit_data['violations'])}")

        # Phase 2: Test Generation
        print("Phase 2: Generating healing tests")
        source_file = Path(unhealthy_codebase) / "calculator.py"
        generate_tool = GenerateTests(
            audit_report=initial_audit_result,
            target_file=str(source_file)
        )

        generation_result = generate_tool.run()
        generation_data = json.loads(generation_result)

        assert generation_data["status"] == "success"
        assert generation_data["tests_generated"] > 0
        assert generation_data["violations_addressed"] > 0

        print(f"Generated {generation_data['tests_generated']} tests")
        print(f"Addressed {generation_data['violations_addressed']} violations")

        # Phase 3: Verification Audit
        print("Phase 3: Re-auditing after test generation")
        verification_tool = AnalyzeCodebase(target_path=unhealthy_codebase, mode="verification")
        final_audit_result = verification_tool.run()
        final_audit_data = json.loads(final_audit_result)

        final_qt_score = final_audit_data["qt_score"]
        print(f"Final Q(T) score: {final_qt_score}")
        print(f"Final violations: {len(final_audit_data['violations'])}")

        # Phase 4: Healing Verification
        print("Phase 4: Verifying healing effectiveness")

        # Q(T) score should improve
        improvement = final_qt_score - initial_qt_score
        assert improvement > 0, f"Q(T) should improve: {initial_qt_score} → {final_qt_score}"

        # Should have fewer critical violations
        initial_critical = len([v for v in initial_audit_data["violations"] if v["severity"] == "critical"])
        final_critical = len([v for v in final_audit_data["violations"] if v["severity"] == "critical"])

        print(f"Critical violations: {initial_critical} → {final_critical}")

        # Verify generated test file exists and is substantial
        generated_test_file = generation_data["test_file"]
        assert os.path.exists(generated_test_file)

        with open(generated_test_file, 'r') as f:
            test_content = f.read()

        assert len(test_content) > 1000, "Generated tests should be substantial"
        assert "def test_" in test_content
        assert "assert" in test_content

        return {
            "initial_qt": initial_qt_score,
            "final_qt": final_qt_score,
            "improvement": improvement,
            "tests_generated": generation_data["tests_generated"],
            "violations_addressed": generation_data["violations_addressed"]
        }

    def test_iterative_healing_process(self, unhealthy_codebase):
        """Test multiple iterations of the healing process."""
        current_codebase = unhealthy_codebase
        qt_scores = []
        iterations = 3

        for iteration in range(iterations):
            print(f"Healing iteration {iteration + 1}")

            # Audit
            audit_tool = AnalyzeCodebase(target_path=current_codebase, mode="full")
            audit_result = audit_tool.run()
            audit_data = json.loads(audit_result)

            qt_score = audit_data["qt_score"]
            qt_scores.append(qt_score)
            print(f"Iteration {iteration + 1} Q(T): {qt_score}")

            # If we have high-severity violations, generate more tests
            high_severity_violations = [
                v for v in audit_data["violations"]
                if v["severity"] in ["critical", "high"]
            ]

            if high_severity_violations:
                # Generate tests for remaining violations
                source_file = Path(current_codebase) / "calculator.py"
                generate_tool = GenerateTests(
                    audit_report=audit_result,
                    target_file=str(source_file)
                )

                generation_result = generate_tool.run()
                generation_data = json.loads(generation_result)

                print(f"Iteration {iteration + 1}: Generated {generation_data.get('tests_generated', 0)} tests")

        # Verify progressive improvement
        for i in range(1, len(qt_scores)):
            # Allow for some fluctuation but overall trend should be positive
            if i == len(qt_scores) - 1:  # Last iteration should definitely be better
                assert qt_scores[i] >= qt_scores[0], f"Final Q(T) should be better than initial: {qt_scores}"

        print(f"Q(T) progression: {qt_scores}")

    def test_multi_file_codebase_healing(self):
        """Test healing of a multi-file codebase."""
        with tempfile.TemporaryDirectory() as temp_dir:
            # Create multiple source files
            math_utils = Path(temp_dir) / "math_utils.py"
            math_utils.write_text('''
def factorial(n):
    """Calculate factorial."""
    if n <= 1:
        return 1
    return n * factorial(n - 1)

def fibonacci(n):
    """Calculate nth Fibonacci number."""
    if n <= 1:
        return n
    return fibonacci(n - 1) + fibonacci(n - 2)
''')

            string_utils = Path(temp_dir) / "string_utils.py"
            string_utils.write_text('''
def reverse_string(s):
    """Reverse a string."""
    return s[::-1]

def count_words(text):
    """Count words in text."""
    if not text:
        return 0
    return len(text.split())

def capitalize_words(text):
    """Capitalize each word."""
    return ' '.join(word.capitalize() for word in text.split())
''')

            # Create minimal tests
            test_math = Path(temp_dir) / "test_math_utils.py"
            test_math.write_text('''
def test_factorial_basic():
    from math_utils import factorial
    assert factorial(5) == 120
''')

            # Initial audit
            audit_tool = AnalyzeCodebase(target_path=temp_dir, mode="full")
            initial_result = audit_tool.run()
            initial_data = json.loads(initial_result)

            initial_qt = initial_data["qt_score"]
            print(f"Multi-file initial Q(T): {initial_qt}")

            # Generate tests for each file
            for source_file in [math_utils, string_utils]:
                generate_tool = GenerateTests(
                    audit_report=initial_result,
                    target_file=str(source_file)
                )

                generation_result = generate_tool.run()
                generation_data = json.loads(generation_result)

                assert generation_data["status"] == "success"
                print(f"Generated tests for {source_file.name}: {generation_data['tests_generated']}")

            # Final audit
            final_audit_tool = AnalyzeCodebase(target_path=temp_dir, mode="verification")
            final_result = final_audit_tool.run()
            final_data = json.loads(final_result)

            final_qt = final_data["qt_score"]
            print(f"Multi-file final Q(T): {final_qt}")

            # Should show improvement
            assert final_qt > initial_qt

    def test_edge_case_healing_robustness(self):
        """Test healing process robustness with edge cases."""
        edge_cases = [
            # Empty file
            ("empty.py", ""),

            # Only comments
            ("comments_only.py", '''
# This file only has comments
# No actual code to test
# Should handle gracefully
'''),

            # Single function
            ("single_function.py", '''
def hello():
    """Single function file."""
    return "hello"
'''),

            # Complex edge case
            ("complex_edge.py", '''
def edge_case_function(param=None):
    """Function with default parameters and edge cases."""
    if param is None:
        return []

    if isinstance(param, str):
        if not param:
            return ["empty"]
        return param.split()

    if isinstance(param, (int, float)):
        if param == 0:
            return [0]
        elif param < 0:
            return list(range(param, 0))
        else:
            return list(range(param))

    return [str(param)]
''')
        ]

        for filename, content in edge_cases:
            with tempfile.TemporaryDirectory() as temp_dir:
                source_file = Path(temp_dir) / filename
                source_file.write_text(content)

                # Audit
                audit_tool = AnalyzeCodebase(target_path=temp_dir)
                audit_result = audit_tool.run()
                audit_data = json.loads(audit_result)

                print(f"Edge case {filename} Q(T): {audit_data['qt_score']}")

                # Generate tests
                if audit_data["codebase_analysis"]["total_behaviors"] > 0:
                    generate_tool = GenerateTests(
                        audit_report=audit_result,
                        target_file=str(source_file)
                    )

                    generation_result = generate_tool.run()
                    generation_data = json.loads(generation_result)

                    # Should handle gracefully
                    assert generation_data["status"] == "success"
                    print(f"Edge case {filename}: Generated {generation_data['tests_generated']} tests")


class TestCodeHealerPerformanceIntegration:
    """Performance testing for the complete healing pipeline."""

    def test_large_codebase_healing_performance(self):
        """Test healing performance on larger codebases."""
        with tempfile.TemporaryDirectory() as temp_dir:
            # Create a larger codebase
            num_files = 10
            functions_per_file = 20

            for file_idx in range(num_files):
                file_path = Path(temp_dir) / f"module_{file_idx}.py"
                functions = []

                for func_idx in range(functions_per_file):
                    functions.append(f'''
def function_{file_idx}_{func_idx}(param_a, param_b=None):
    """Function {func_idx} in module {file_idx}."""
    if param_b is None:
        param_b = param_a

    if param_a == 0:
        return param_b

    return param_a + param_b + {func_idx}

class Class_{file_idx}_{func_idx}:
    """Class {func_idx} in module {file_idx}."""

    def __init__(self, value={func_idx}):
        self.value = value

    def method_{func_idx}(self):
        """Method {func_idx}."""
        return self.value * {func_idx + 1}
''')

                file_path.write_text('\n'.join(functions))

            # Create some basic tests
            test_file = Path(temp_dir) / "test_basic.py"
            test_file.write_text('''
def test_basic():
    """Basic test."""
    assert True
''')

            # Time the complete healing process
            start_time = time.time()

            # Audit
            audit_tool = AnalyzeCodebase(target_path=temp_dir)
            audit_result = audit_tool.run()
            audit_data = json.loads(audit_result)

            audit_time = time.time() - start_time
            print(f"Large codebase audit time: {audit_time:.2f}s")
            print(f"Behaviors found: {audit_data['codebase_analysis']['total_behaviors']}")

            # Generate tests for first few files only (to keep test time reasonable)
            generation_start = time.time()

            for file_idx in range(min(3, num_files)):  # Test first 3 files
                source_file = Path(temp_dir) / f"module_{file_idx}.py"
                generate_tool = GenerateTests(
                    audit_report=audit_result,
                    target_file=str(source_file)
                )

                generation_result = generate_tool.run()
                generation_data = json.loads(generation_result)

                assert generation_data["status"] == "success"

            generation_time = time.time() - generation_start
            print(f"Test generation time: {generation_time:.2f}s")

            total_time = time.time() - start_time
            print(f"Total healing time: {total_time:.2f}s")

            # Performance should be reasonable
            assert audit_time < 30, f"Audit should complete in reasonable time: {audit_time}s"
            assert generation_time < 60, f"Generation should complete in reasonable time: {generation_time}s"

    def test_concurrent_healing_safety(self):
        """Test safety of concurrent healing operations."""
        import threading
        import queue

        with tempfile.TemporaryDirectory() as temp_dir:
            # Create multiple small codebases
            for i in range(3):
                subdir = Path(temp_dir) / f"codebase_{i}"
                subdir.mkdir()

                source_file = subdir / f"module_{i}.py"
                source_file.write_text(f'''
def function_{i}():
    """Function {i}."""
    return {i}

class Class_{i}:
    """Class {i}."""

    def method_{i}(self):
        """Method {i}."""
        return {i} * 2
''')

            results_queue = queue.Queue()

            def healing_worker(codebase_path):
                """Worker function for concurrent healing."""
                try:
                    # Audit
                    audit_tool = AnalyzeCodebase(target_path=str(codebase_path))
                    audit_result = audit_tool.run()
                    audit_data = json.loads(audit_result)

                    # Generate tests
                    source_files = list(codebase_path.glob("*.py"))
                    if source_files:
                        generate_tool = GenerateTests(
                            audit_report=audit_result,
                            target_file=str(source_files[0])
                        )

                        generation_result = generate_tool.run()
                        generation_data = json.loads(generation_result)

                        results_queue.put({
                            "codebase": str(codebase_path),
                            "status": "success",
                            "qt_score": audit_data["qt_score"],
                            "tests_generated": generation_data["tests_generated"]
                        })
                    else:
                        results_queue.put({
                            "codebase": str(codebase_path),
                            "status": "no_files"
                        })

                except Exception as e:
                    results_queue.put({
                        "codebase": str(codebase_path),
                        "status": "error",
                        "error": str(e)
                    })

            # Start concurrent healing operations
            threads = []
            for i in range(3):
                codebase_path = Path(temp_dir) / f"codebase_{i}"
                thread = threading.Thread(target=healing_worker, args=(codebase_path,))
                threads.append(thread)
                thread.start()

            # Wait for completion
            for thread in threads:
                thread.join(timeout=30)
                assert not thread.is_alive(), "Healing thread should complete"

            # Verify all completed successfully
            results = []
            while not results_queue.empty():
                results.append(results_queue.get())

            assert len(results) == 3
            for result in results:
                assert result["status"] in ["success", "no_files"], f"Healing should succeed: {result}"

    def test_memory_usage_during_healing(self):
        """Test memory usage stays reasonable during healing."""
        import gc
        import sys

        with tempfile.TemporaryDirectory() as temp_dir:
            # Create a moderately complex codebase
            for i in range(5):
                source_file = Path(temp_dir) / f"memory_test_{i}.py"
                functions = []

                for j in range(30):
                    functions.append(f'''
def memory_function_{i}_{j}(data):
    """Memory function {i}_{j}."""
    if not data:
        return []

    result = []
    for item in data:
        if isinstance(item, str):
            result.append(item.upper())
        elif isinstance(item, (int, float)):
            result.append(item * {j + 1})
        else:
            result.append(str(item))

    return result
''')

                source_file.write_text('\n'.join(functions))

            # Measure initial memory
            gc.collect()
            initial_objects = len(gc.get_objects())

            # Perform healing
            audit_tool = AnalyzeCodebase(target_path=temp_dir)
            audit_result = audit_tool.run()

            # Generate tests for a couple of files
            for i in range(2):
                source_file = Path(temp_dir) / f"memory_test_{i}.py"
                generate_tool = GenerateTests(
                    audit_report=audit_result,
                    target_file=str(source_file)
                )

                generation_result = generate_tool.run()
                generation_data = json.loads(generation_result)

                assert generation_data["status"] == "success"

                # Clear references
                del generate_tool

            # Measure final memory
            gc.collect()
            final_objects = len(gc.get_objects())

            memory_growth = final_objects - initial_objects
            print(f"Memory growth during healing: {memory_growth} objects")

            # Memory growth should be reasonable
            assert memory_growth < 5000, f"Memory growth should be reasonable: {memory_growth}"


class TestCodeHealerPhilosophicalCompleteness:
    """Test the philosophical aspects of self-healing and meta-testing."""

    def test_framework_heals_itself(self):
        """Test that the framework can heal its own components."""
        # This test uses the actual auditor and test generator files
        # to test if they can generate healing tests for themselves

        auditor_file = Path(__file__).parent.parent / "auditor_agent" / "auditor_agent.py"
        generator_file = Path(__file__).parent.parent / "test_generator_agent" / "test_generator_agent.py"

        # Test healing the auditor
        if auditor_file.exists():
            audit_tool = AnalyzeCodebase(target_path=str(auditor_file.parent))
            audit_result = audit_tool.run()
            audit_data = json.loads(audit_result)

            print(f"Auditor self-audit Q(T): {audit_data['qt_score']}")

            # Try to generate tests for the auditor
            generate_tool = GenerateTests(
                audit_report=audit_result,
                target_file=str(auditor_file)
            )

            with tempfile.TemporaryDirectory() as temp_dir:
                # Modify tool to write to temp location for testing
                original_get_test_file_path = generate_tool._get_test_file_path

                def mock_get_test_file_path(source_file):
                    return str(Path(temp_dir) / "test_auditor_self_heal.py")

                generate_tool._get_test_file_path = mock_get_test_file_path

                generation_result = generate_tool.run()
                generation_data = json.loads(generation_result)

                # Should succeed in generating tests for itself
                assert generation_data["status"] == "success"
                print(f"Self-healing auditor: {generation_data['tests_generated']} tests generated")

        # Test healing the test generator
        if generator_file.exists():
            audit_tool = AnalyzeCodebase(target_path=str(generator_file.parent))
            audit_result = audit_tool.run()
            audit_data = json.loads(audit_result)

            print(f"Generator self-audit Q(T): {audit_data['qt_score']}")

            generate_tool = GenerateTests(
                audit_report=audit_result,
                target_file=str(generator_file)
            )

            with tempfile.TemporaryDirectory() as temp_dir:
                original_get_test_file_path = generate_tool._get_test_file_path

                def mock_get_test_file_path(source_file):
                    return str(Path(temp_dir) / "test_generator_self_heal.py")

                generate_tool._get_test_file_path = mock_get_test_file_path

                generation_result = generate_tool.run()
                generation_data = json.loads(generation_result)

                assert generation_data["status"] == "success"
                print(f"Self-healing generator: {generation_data['tests_generated']} tests generated")

    def test_infinite_recursion_prevention_in_practice(self):
        """Test that self-healing doesn't create infinite recursion."""
        with tempfile.TemporaryDirectory() as temp_dir:
            # Create a self-referential test generator
            self_generator = Path(temp_dir) / "self_generator.py"
            self_generator.write_text('''
def generate_test_for_self():
    """Generate test for this very function."""
    return """
def test_generate_test_for_self():
    from self_generator import generate_test_for_self
    result = generate_test_for_self()
    assert "def test_" in result
"""

def analyze_and_generate(target_function):
    """Analyze and generate tests for a function."""
    if target_function == "generate_test_for_self":
        return generate_test_for_self()
    elif target_function == "analyze_and_generate":
        # This could cause infinite recursion
        return analyze_and_generate("analyze_and_generate")
    else:
        return f"def test_{target_function}(): pass"
''')

            # Perform healing - should not hang
            start_time = time.time()

            audit_tool = AnalyzeCodebase(target_path=temp_dir)
            audit_result = audit_tool.run()

            generate_tool = GenerateTests(
                audit_report=audit_result,
                target_file=str(self_generator)
            )

            generation_result = generate_tool.run()
            generation_data = json.loads(generation_result)

            end_time = time.time()

            # Should complete quickly without infinite recursion
            assert (end_time - start_time) < 10, "Should not hang in infinite recursion"
            assert generation_data["status"] == "success"

    def test_quality_assessment_convergence(self):
        """Test that quality assessment eventually converges."""
        with tempfile.TemporaryDirectory() as temp_dir:
            # Create a codebase that can be incrementally improved
            source_file = Path(temp_dir) / "convergence_test.py"
            source_file.write_text('''
def simple_function():
    """Simple function for convergence testing."""
    return "simple"

def another_function(param):
    """Another function with parameter."""
    return param * 2

class SimpleClass:
    """Simple class for convergence testing."""

    def method(self):
        """Simple method."""
        return "method"
''')

            qt_scores = []
            max_iterations = 5
            convergence_threshold = 0.05  # Consider converged if change < 5%

            for iteration in range(max_iterations):
                # Audit
                audit_tool = AnalyzeCodebase(target_path=temp_dir)
                audit_result = audit_tool.run()
                audit_data = json.loads(audit_result)

                qt_score = audit_data["qt_score"]
                qt_scores.append(qt_score)

                print(f"Convergence iteration {iteration + 1}: Q(T) = {qt_score}")

                # Check for convergence
                if len(qt_scores) >= 2:
                    improvement = qt_scores[-1] - qt_scores[-2]
                    if abs(improvement) < convergence_threshold:
                        print(f"Converged after {iteration + 1} iterations")
                        break

                # Generate more tests
                if audit_data["violations"]:
                    generate_tool = GenerateTests(
                        audit_report=audit_result,
                        target_file=str(source_file)
                    )

                    generation_result = generate_tool.run()
                    generation_data = json.loads(generation_result)

                    if generation_data["tests_generated"] == 0:
                        print("No more tests to generate")
                        break

            # Should show improvement over iterations
            if len(qt_scores) > 1:
                final_improvement = qt_scores[-1] - qt_scores[0]
                print(f"Total Q(T) improvement: {final_improvement}")
                assert final_improvement >= 0, "Q(T) should not decrease over iterations"